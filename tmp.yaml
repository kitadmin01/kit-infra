NAME: analytickit
LAST DEPLOYED: Mon Nov 14 14:22:33 2022
NAMESPACE: default
STATUS: pending-install
REVISION: 1
TEST SUITE: None
USER-SUPPLIED VALUES:
busybox:
  image: busybox:1.34
  pullPolicy: IfNotPresent
cert-manager:
  email: null
  enabled: false
  installCRDs: true
  podDnsConfig:
    nameservers:
    - 8.8.8.8
    - 1.1.1.1
    - 208.67.222.222
  podDnsPolicy: None
clickhouse:
  affinity: {}
  allowedNetworkIps:
  - 10.0.0.0/8
  - 172.16.0.0/12
  - 192.168.0.0/16
  backup:
    backup_password: backup_password
    backup_schedule: 0 0 * * *
    backup_user: backup
    clickhouse_services: chi-analytickit-analytickit-0-0
    enabled: false
    env:
    - name: LOG_LEVEL
      value: debug
    - name: ALLOW_EMPTY_BACKUPS
      value: "true"
    - name: API_LISTEN
      value: 0.0.0.0:7171
    - name: API_CREATE_INTEGRATION_TABLES
      value: "true"
    - name: BACKUPS_TO_KEEP_REMOTE
      value: "0"
    existingSecret: ""
    existingSecretPasswordKey: ""
    image:
      pullPolicy: IfNotPresent
      repository: altinity/clickhouse-backup
      tag: 1.5.0
  client:
    image:
      pullPolicy: IfNotPresent
      repository: clickhouse/clickhouse-server
      tag: 22.3.13.80
  cluster: analytickit
  database: analytickit
  defaultProfiles:
    default/allow_experimental_window_functions: "1"
    default/allow_nondeterministic_mutations: "1"
  defaultSettings:
    default_database: analytickit
    format_schema_path: /etc/clickhouse-server/config.d/
  enabled: true
  existingSecret: ""
  existingSecretPasswordKey: ""
  image:
    pullPolicy: IfNotPresent
    repository: clickhouse/clickhouse-server
    tag: 22.3.13.80
  layout:
    replicasCount: 1
    shardsCount: 1
  namespace: null
  password: a1f31e03-c88e-4ca6-a2df-ad49183d15d9
  persistence:
    enabled: true
    existingClaim: ""
    size: 20Gi
    storageClass: null
  podAnnotations: null
  podDistribution: null
  profiles: {}
  resources: {}
  secure: false
  securityContext:
    enabled: true
    fsGroup: 101
    runAsGroup: 101
    runAsUser: 101
  serviceType: ClusterIP
  settings: {}
  tolerations: []
  user: admin
  verify: false
cloud: aws
cloudwatch:
  clusterName: null
  enabled: false
  fluentBit:
    port: 2020
    readHead: "On"
    readTail: "Off"
    server: "On"
  region: null
email:
  existingSecret: ""
  existingSecretKey: ""
  from_email: null
  host: null
  password: null
  port: null
  use_ssl: null
  use_tls: true
  user: null
env: []
eventrouter:
  enabled: false
  image:
    pullPolicy: IfNotPresent
    repository: gcr.io/heptio-images/eventrouter
    tag: v0.3
  resources: {}
events:
  enabled: true
  hpa:
    behavior: null
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  podSecurityContext:
    enabled: false
  replicacount: 1
  securityContext:
    enabled: false
externalClickhouse:
  cluster: null
  database: analytickit
  existingSecret: null
  existingSecretPasswordKey: null
  host: null
  password: null
  secure: false
  user: null
  verify: false
externalKafka:
  brokers: []
externalObjectStorage:
  bucket: null
  endpoint: null
  existingSecret: null
  host: null
  port: null
externalPostgresql:
  existingSecret: null
  existingSecretPasswordKey: postgresql-password
  postgresqlDatabase: null
  postgresqlHost: null
  postgresqlPassword: null
  postgresqlPort: 5432
  postgresqlUsername: null
externalRedis:
  existingSecret: ""
  existingSecretPasswordKey: ""
  host: ""
  password: ""
  port: 6379
externalStatsd:
  host: null
  port: null
grafana:
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - access: proxy
        isDefault: true
        jsonData:
          timeInterval: 60s
        name: Prometheus
        type: prometheus
        url: http://analytickit-prometheus-server
      - access: proxy
        isDefault: false
        name: Loki
        type: loki
        url: http://analytickit-loki-read:3100
      - access: proxy
        isDefault: false
        jsonData:
          implementation: prometheus
        name: Alertmanager
        type: alertmanager
        url: http://analytickit-prometheus-alertmanager
  enabled: false
  sidecar:
    dashboards:
      enabled: true
      folderAnnotation: grafana_folder
      label: grafana_dashboard
      provider:
        foldersFromFilesStructure: true
hooks:
  affinity: {}
  migrate:
    env: []
    resources: {}
  nodeSelector: {}
  tolerations: []
image:
  default: :release-1.41.3
  pullPolicy: IfNotPresent
  repository: analytickit/analytickit
  sha: null
  tag: null
ingress:
  annotations: {}
  enabled: true
  gcp:
    forceHttps: true
    ip_name: analytickit
    secretName: ""
  hostname: null
  letsencrypt: null
  nginx:
    enabled: false
    redirectToTLS: true
  secretName: null
  type: null
ingress-nginx:
  controller:
    config:
      log-format-escape-json: "true"
      log-format-upstream: '{ "time": "$time_iso8601", "remote_addr": "$proxy_protocol_addr",
        "request_id": "$request_id", "correlation_id": "$request_id", "remote_user":
        "$remote_user", "bytes_sent": $bytes_sent, "request_time": $request_time,
        "status": $status, "host": "$host", "request_proto": "$server_protocol", "uri":
        "$uri", "request_query": "$args", "request_length": $request_length, "duration":
        $request_time, "method": "$request_method", "http_referrer": "$http_referer",
        "http_user_agent": "$http_user_agent", "http_x_forwarded_for": "$http_x_forwarded_for"
        }'
      use-forwarded-headers: "true"
    proxySetHeaders:
      X-Correlation-ID: $request_id
installCustomStorageClass: false
kafka:
  enabled: true
  externalZookeeper:
    servers:
    - analytickit-analytickit-zookeeper:2181
  fullnameOverride: ""
  logRetentionBytes: _15_000_000_000
  logRetentionHours: 24
  nameOverride: analytickit-kafka
  numPartitions: 1
  persistence:
    enabled: true
    size: 20Gi
  zookeeper:
    enabled: false
loki:
  enabled: false
  fullnameOverride: analytickit-loki
  gateway:
    enabled: false
  loki:
    auth_enabled: false
    commonConfig:
      replication_factor: 1
  minio:
    drivesPerNode: 2
    enabled: true
    replicas: 1
  monitoring:
    alerts:
      enabled: false
    dashboards:
      enabled: false
    rules:
      enabled: false
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
    serviceMonitor:
      enabled: false
  nameOverride: analytickit-loki
  read:
    replicas: 1
  write:
    replicas: 1
migrate:
  enabled: true
minio:
  auth:
    existingSecret: null
    rootPassword: root-password-change-me-please
    rootUser: root-user
  defaultBuckets: analytickit
  disableWebUI: true
  enabled: false
  persistence:
    enabled: true
  podAnnotations: null
  service:
    ports:
      api: "19000"
      console: "19001"
notificationEmail: null
pgbouncer:
  affinity: {}
  enabled: true
  env:
  - name: PGBOUNCER_PORT
    value: "6543"
  - name: PGBOUNCER_MAX_CLIENT_CONN
    value: "1000"
  - name: PGBOUNCER_POOL_MODE
    value: transaction
  - name: PGBOUNCER_IGNORE_STARTUP_PARAMETERS
    value: extra_float_digits
  exporter:
    enabled: false
    image:
      pullPolicy: IfNotPresent
      repository: prometheuscommunity/pgbouncer-exporter
      tag: v0.4.1
    port: 9127
    resources: {}
    securityContext:
      enabled: false
  hpa:
    behavior: null
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  image:
    pullPolicy: IfNotPresent
    repository: bitnami/pgbouncer
    tag: 1.17.0
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 60
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podAnnotations: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 2
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  service:
    annotations: {}
    type: ClusterIP
  tolerations: []
plugins:
  affinity: {}
  enabled: true
  env: []
  hpa:
    behavior: null
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  sentryDSN: null
  tolerations: []
pluginsAsync:
  affinity: {}
  enabled: false
  env: []
  hpa:
    behavior: null
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  sentryDSN: null
  tolerations: []
pluginsExports:
  affinity: {}
  enabled: false
  env: []
  hpa:
    behavior: null
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  sentryDSN: null
  tolerations: []
pluginsIngestion:
  affinity: {}
  enabled: false
  env: []
  hpa:
    behavior: null
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  sentryDSN: null
  tolerations: []
pluginsJobs:
  affinity: {}
  enabled: false
  env: []
  hpa:
    behavior: null
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  sentryDSN: null
  tolerations: []
pluginsScheduler:
  affinity: {}
  enabled: false
  env: []
  hpa:
    behavior: null
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  sentryDSN: null
  tolerations: []
postgresql:
  enabled: true
  nameOverride: analytickit-postgresql
  persistence:
    enabled: true
    size: 10Gi
  postgresqlDatabase: analytickit
  postgresqlPassword: postgres
posthogSecretKey:
  existingSecret: null
  existingSecretKey: analytickit-secret
prometheus:
  alertmanager:
    enabled: false
    podAnnotations: {}
  enabled: false
  pushgateway:
    enabled: false
  serverFiles:
    alerting_rules.yml:
      groups:
      - name: Kubernetes
        rules:
        - alert: KubernetesNodeReady
          annotations:
            description: Node {{ $labels.node }} has been unready for a long time
            summary: Kubernetes Node ready (instance {{ $labels.instance }})
          expr: kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 10m
          labels:
            severity: critical
        - alert: KubernetesMemoryPressure
          annotations:
            description: '{{ $labels.node }} has MemoryPressure condition'
            summary: Kubernetes memory pressure (instance {{ $labels.instance }})
          expr: kube_node_status_condition{condition="MemoryPressure",status="true"}
            == 1
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesDiskPressure
          annotations:
            description: '{{ $labels.node }} has DiskPressure condition'
            summary: Kubernetes disk pressure (instance {{ $labels.instance }})
          expr: kube_node_status_condition{condition="DiskPressure",status="true"}
            == 1
          for: 10m
          labels:
            severity: critical
        - alert: KubernetesOutOfDisk
          annotations:
            description: '{{ $labels.node }} has OutOfDisk condition'
            summary: Kubernetes out of disk (instance {{ $labels.instance }})
          expr: kube_node_status_condition{condition="OutOfDisk",status="true"} ==
            1
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesOutOfCapacity
          annotations:
            description: '{{ $labels.node }} is out of capacity'
            summary: Kubernetes out of capacity (instance {{ $labels.instance }})
          expr: sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid)
            group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node)
            (kube_node_status_allocatable{resource="pods"}) * 100 > 90
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesContainerOomKiller
          annotations:
            description: Container {{ $labels.container }} in pod {{ $labels.namespace
              }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last
              10 minutes.
            summary: Kubernetes container oom killer (instance {{ $labels.instance
              }})
          expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total
            offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m])
            == 1
          for: 0m
          labels:
            severity: critical
        - alert: KubernetesJobFailed
          annotations:
            description: Job {{$labels.namespace}}/{{$labels.exported_job}} failed
              to complete
            summary: Kubernetes Job failed (instance {{ $labels.instance }})
          expr: kube_job_status_failed > 0
          for: 0m
          labels:
            severity: warning
        - alert: KubernetesCronjobSuspended
          annotations:
            description: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is
              suspended
            summary: Kubernetes CronJob suspended (instance {{ $labels.instance }})
          expr: kube_cronjob_spec_suspend != 0
          for: 0m
          labels:
            severity: warning
        - alert: KubernetesPersistentvolumeclaimPending
          annotations:
            description: PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim
              }} is pending
            summary: Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance
              }})
          expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
          for: 2m
          labels:
            severity: warning
        - alert: KubernetesVolumeOutOfDiskSpace
          annotations:
            description: Volume is almost full (< 10% left)
            summary: Kubernetes Volume out of disk space (instance {{ $labels.instance
              }})
          expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes
            * 100 < 10
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesVolumeFullInFourDays
          annotations:
            description: '{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim
              }} is expected to fill up within four days. Currently {{ $value | humanize
              }}% is available.'
            summary: Kubernetes Volume full in four days (instance {{ $labels.instance
              }})
          expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 *
            3600) < 0
          for: 0m
          labels:
            severity: critical
        - alert: KubernetesPersistentvolumeError
          annotations:
            description: Persistent volume is in bad state
            summary: Kubernetes PersistentVolume error (instance {{ $labels.instance
              }})
          expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"}
            > 0
          for: 0m
          labels:
            severity: critical
        - alert: KubernetesStatefulsetDown
          annotations:
            description: A StatefulSet went down
            summary: Kubernetes StatefulSet down (instance {{ $labels.instance }})
          expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current)
            != 1
          for: 1m
          labels:
            severity: critical
        - alert: KubernetesHpaScalingAbility
          annotations:
            description: Pod is unable to scale
            summary: Kubernetes HPA scaling ability (instance {{ $labels.instance
              }})
          expr: kube_horizontalpodautoscaler_status_condition{status="false", condition="AbleToScale"}
            == 1
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesHpaMetricAvailability
          annotations:
            description: HPA is not able to collect metrics
            summary: Kubernetes HPA metric availability (instance {{ $labels.instance
              }})
          expr: kube_horizontalpodautoscaler_status_condition{status="false", condition="ScalingActive"}
            == 1
          for: 5m
          labels:
            severity: critical
        - alert: KubernetesHpaScaleCapability
          annotations:
            description: The maximum number of desired Pods has been hit
            summary: Kubernetes HPA scale capability (instance {{ $labels.instance
              }})
          expr: kube_horizontalpodautoscaler_status_desired_replicas >= kube_horizontalpodautoscaler_spec_max_replicas
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesPodNotHealthy
          annotations:
            description: Pod has been in a non-ready state for longer than 15 minutes.
            summary: Kubernetes Pod not healthy (instance {{ $labels.instance }})
          expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})
            > 0
          for: 15m
          labels:
            severity: warning
        - alert: KubernetesPodCrashLooping
          annotations:
            description: Pod {{ $labels.pod }} is crash looping
            summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
          expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
          for: 2m
          labels:
            severity: warning
        - alert: KubernetesReplicassetMismatch
          annotations:
            description: |
              The number of ready pods in the Deployment's replicaset does not match the desired number.
            summary: Kubernetes ReplicasSet mismatch (instance {{ $labels.instance
              }})
          expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
          for: 10m
          labels:
            severity: warning
        - alert: KubernetesDeploymentReplicasMismatch
          annotations:
            description: |
              The number of ready pods in the Deployment does not match the desired number.
            summary: Kubernetes Deployment replicas mismatch (instance {{ $labels.instance
              }})
          expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
          for: 10m
          labels:
            severity: warning
        - alert: KubernetesStatefulsetReplicasMismatch
          annotations:
            description: |
              The number of ready pods in the StatefulSet does not match the desired number.
            summary: Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance
              }})
          expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
          for: 10m
          labels:
            severity: warning
        - alert: KubernetesDeploymentGenerationMismatch
          annotations:
            description: A Deployment has failed but has not been rolled back.
            summary: Kubernetes Deployment generation mismatch (instance {{ $labels.instance
              }})
          expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
          for: 10m
          labels:
            severity: critical
        - alert: KubernetesStatefulsetGenerationMismatch
          annotations:
            description: A StatefulSet has failed but has not been rolled back.
            summary: Kubernetes StatefulSet generation mismatch (instance {{ $labels.instance
              }})
          expr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
          for: 10m
          labels:
            severity: critical
        - alert: KubernetesStatefulsetUpdateNotRolledOut
          annotations:
            description: StatefulSet update has not been rolled out.
            summary: Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance
              }})
          expr: max without (revision) (kube_statefulset_status_current_revision unless
            kube_statefulset_status_update_revision) * (kube_statefulset_replicas
            != kube_statefulset_status_replicas_updated)
          for: 10m
          labels:
            severity: warning
        - alert: KubernetesDaemonsetRolloutStuck
          annotations:
            description: Some Pods of DaemonSet are not scheduled or not ready
            summary: Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance
              }})
          expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled
            * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled
            > 0
          for: 10m
          labels:
            severity: warning
        - alert: KubernetesDaemonsetMisscheduled
          annotations:
            description: Some DaemonSet Pods are running where they are not supposed
              to run
            summary: Kubernetes DaemonSet misscheduled (instance {{ $labels.instance
              }})
          expr: kube_daemonset_status_number_misscheduled > 0
          for: 5m
          labels:
            severity: critical
        - alert: KubernetesCronjobTooLong
          annotations:
            description: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is
              taking more than 1h to complete.
            summary: Kubernetes CronJob too long (instance {{ $labels.instance }})
          expr: time() - kube_cronjob_next_schedule_time > 3600
          for: 0m
          labels:
            severity: warning
        - alert: KubernetesJobSlowCompletion
          annotations:
            description: Kubernetes Job {{ $labels.namespace }}/{{ $labels.job_name
              }} did not complete in time.
            summary: Kubernetes job slow completion (instance {{ $labels.instance
              }})
          expr: kube_job_spec_completions - kube_job_status_succeeded > 0
          for: 12h
          labels:
            severity: critical
        - alert: KubernetesApiServerErrors
          annotations:
            description: Kubernetes API server is experiencing high error rate
            summary: Kubernetes API server errors (instance {{ $labels.instance }})
          expr: sum(rate(apiserver_request_total{job="apiserver",code=~"^(?:5..)$"}[1m]))
            / sum(rate(apiserver_request_total{job="apiserver"}[1m])) * 100 > 3
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesApiClientErrors
          annotations:
            description: Kubernetes API client is experiencing high error rate
            summary: Kubernetes API client errors (instance {{ $labels.instance }})
          expr: (sum(rate(rest_client_requests_total{code=~"(4|5).."}[1m])) by (instance,
            job) / sum(rate(rest_client_requests_total[1m])) by (instance, job)) *
            100 > 1
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesClientCertificateExpiresNextWeek
          annotations:
            description: A client certificate used to authenticate to the apiserver
              is expiring next week.
            summary: Kubernetes client certificate expires next week (instance {{
              $labels.instance }})
          expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"}
            > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m])))
            < 7*24*60*60
          for: 0m
          labels:
            severity: warning
        - alert: KubernetesClientCertificateExpiresSoon
          annotations:
            description: A client certificate used to authenticate to the apiserver
              is expiring in less than 24.0 hours.
            summary: Kubernetes client certificate expires soon (instance {{ $labels.instance
              }})
          expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"}
            > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m])))
            < 24*60*60
          for: 0m
          labels:
            severity: critical
        - alert: KubernetesApiServerLatency
          annotations:
            description: Kubernetes API server has a 99th percentile latency of {{
              $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.
            summary: Kubernetes API server latency (instance {{ $labels.instance }})
          expr: histogram_quantile(0.99, sum(rate(apiserver_request_latencies_bucket{subresource!="log",verb!~"^(?:CONNECT|WATCHLIST|WATCH|PROXY)$"}
            [10m])) WITHOUT (instance, resource)) / 1e+06 > 1
          for: 2m
          labels:
            severity: warning
      - name: Loki
        rules:
        - alert: LokiProcessTooManyRestarts
          annotations:
            description: A loki process had too many restarts (target {{ $labels.instance
              }})
            summary: Loki process too many restarts (instance {{ $labels.instance
              }})
          expr: changes(process_start_time_seconds{app="loki"}[15m]) > 2
          for: 0m
          labels:
            severity: warning
        - alert: LokiRequestErrors
          annotations:
            description: The {{ $labels.job }} and {{ $labels.route }} are experiencing
              errors
            summary: Loki request errors (instance {{ $labels.instance }})
          expr: 100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m]))
            by (namespace, job, route) / sum(rate(loki_request_duration_seconds_count[1m]))
            by (namespace, job, route) > 10
          for: 15m
          labels:
            severity: warning
        - alert: LokiRequestPanic
          annotations:
            description: The {{ $labels.job }} is experiencing {{ printf "%.2f" $value
              }}% increase of panics
            summary: Loki request panic (instance {{ $labels.instance }})
          expr: sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
          for: 5m
          labels:
            severity: warning
        - alert: LokiRequestLatency
          annotations:
            description: The {{ $labels.job }} {{ $labels.route }} is experiencing
              {{ printf "%.2f" $value }}s 99th percentile latency
            summary: Loki request latency (instance {{ $labels.instance }})
          expr: (histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[5m]))
            by (le))) > 3
          for: 10m
          labels:
            severity: warning
      - name: Promtail
        rules:
        - alert: PromtailRequestErrors
          annotations:
            description: The {{ $labels.job }} {{ $labels.route }} is experiencing
              {{ printf "%.2f" $value }}% errors.
            summary: Promtail request errors (instance {{ $labels.instance }})
          expr: 100 * sum(rate(promtail_request_duration_seconds_count{status_code=~"5..|failed"}[1m]))
            by (namespace, job, route, instance) / sum(rate(promtail_request_duration_seconds_count[1m]))
            by (namespace, job, route, instance) > 10
          for: 5m
          labels:
            severity: critical
        - alert: PromtailRequestLatency
          annotations:
            description: The {{ $labels.job }} {{ $labels.route }} is experiencing
              {{ printf "%.2f" $value }}s 99th percentile latency.
            summary: Promtail request latency (instance {{ $labels.instance }})
          expr: histogram_quantile(0.99, sum(rate(promtail_request_duration_seconds_bucket[5m]))
            by (le)) > 1
          for: 5m
          labels:
            severity: critical
      - name: Prometheus
        rules:
        - alert: PrometheusJobMissing
          annotations:
            description: A Prometheus job has disappeared
            summary: Prometheus job missing (instance {{ $labels.instance }})
          expr: absent(up{job="prometheus"})
          for: 0m
          labels:
            severity: warning
        - alert: PrometheusTargetMissing
          annotations:
            description: A Prometheus target has disappeared. An exporter might be
              crashed.
            summary: Prometheus target missing (instance {{ $labels.instance }})
          expr: up == 0
          for: 5m
          labels:
            severity: critical
        - alert: PrometheusAllTargetsMissing
          annotations:
            description: A Prometheus job does not have living target anymore.
            summary: Prometheus all targets missing (instance {{ $labels.instance
              }})
          expr: sum by (job) (up) == 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusConfigurationReloadFailure
          annotations:
            description: Prometheus configuration reload error
            summary: Prometheus configuration reload failure (instance {{ $labels.instance
              }})
          expr: prometheus_config_last_reload_successful != 1
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTooManyRestarts
          annotations:
            description: Prometheus has restarted more than twice in the last 15 minutes.
              It might be crashlooping.
            summary: Prometheus too many restarts (instance {{ $labels.instance }})
          expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m])
            > 2
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusAlertmanagerJobMissing
          annotations:
            description: A Prometheus AlertManager job has disappeared
            summary: Prometheus AlertManager job missing (instance {{ $labels.instance
              }})
          expr: absent(up{job="kubernetes-pods", app="prometheus", component="alertmanager"})
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusAlertmanagerConfigurationReloadFailure
          annotations:
            description: AlertManager configuration reload error
            summary: Prometheus AlertManager configuration reload failure (instance
              {{ $labels.instance }})
          expr: alertmanager_config_last_reload_successful != 1
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusAlertmanagerConfigNotSynced
          annotations:
            description: Configurations of AlertManager cluster instances are out
              of sync
            summary: Prometheus AlertManager config not synced (instance {{ $labels.instance
              }})
          expr: count(count_values("config_hash", alertmanager_config_hash)) > 1
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusAlertmanagerE2eDeadManSwitch
          annotations:
            description: Prometheus DeadManSwitch is an always-firing alert. It's
              used as an end-to-end test of Prometheus through the Alertmanager.
            summary: Prometheus AlertManager E2E dead man switch (instance {{ $labels.instance
              }})
          expr: vector(1)
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusNotConnectedToAlertmanager
          annotations:
            description: Prometheus cannot connect the alertmanager
            summary: Prometheus not connected to alertmanager (instance {{ $labels.instance
              }})
          expr: prometheus_notifications_alertmanagers_discovered < 1
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusRuleEvaluationFailures
          annotations:
            description: Prometheus encountered {{ $value }} rule evaluation failures,
              leading to potentially ignored alerts.
            summary: Prometheus rule evaluation failures (instance {{ $labels.instance
              }})
          expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTemplateTextExpansionFailures
          annotations:
            description: Prometheus encountered {{ $value }} template text expansion
              failures
            summary: Prometheus template text expansion failures (instance {{ $labels.instance
              }})
          expr: increase(prometheus_template_text_expansion_failures_total[3m]) >
            0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusRuleEvaluationSlow
          annotations:
            description: Prometheus rule evaluation took more time than the scheduled
              interval. It indicates a slower storage backend access or too complex
              query.
            summary: Prometheus rule evaluation slow (instance {{ $labels.instance
              }})
          expr: prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds
          for: 5m
          labels:
            severity: warning
        - alert: PrometheusNotificationsBacklog
          annotations:
            description: The Prometheus notification queue has not been empty for
              10 minutes
            summary: Prometheus notifications backlog (instance {{ $labels.instance
              }})
          expr: min_over_time(prometheus_notifications_queue_length[10m]) > 0
          for: 0m
          labels:
            severity: warning
        - alert: PrometheusAlertmanagerNotificationFailing
          annotations:
            description: Alertmanager is failing sending notifications
            summary: Prometheus AlertManager notification failing (instance {{ $labels.instance
              }})
          expr: rate(alertmanager_notifications_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTargetEmpty
          annotations:
            description: Prometheus has no target in service discovery
            summary: Prometheus target empty (instance {{ $labels.instance }})
          expr: prometheus_sd_discovered_targets == 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTargetScrapingSlow
          annotations:
            description: Prometheus is scraping exporters slowly since it exceeded
              the requested interval time. Your Prometheus server is under-provisioned.
            summary: Prometheus target scraping slow (instance {{ $labels.instance
              }})
          expr: prometheus_target_interval_length_seconds{quantile="0.9"} / on (interval,
            instance, job) prometheus_target_interval_length_seconds{quantile="0.5"}
            > 1.05
          for: 5m
          labels:
            severity: warning
        - alert: PrometheusLargeScrape
          annotations:
            description: Prometheus has many scrapes that exceed the sample limit
            summary: Prometheus large scrape (instance {{ $labels.instance }})
          expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m])
            > 10
          for: 5m
          labels:
            severity: warning
        - alert: PrometheusTargetScrapeDuplicate
          annotations:
            description: Prometheus has many samples rejected due to duplicate timestamps
              but different values
            summary: Prometheus target scrape duplicate (instance {{ $labels.instance
              }})
          expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m])
            > 0
          for: 0m
          labels:
            severity: warning
        - alert: PrometheusTsdbCheckpointCreationFailures
          annotations:
            description: Prometheus encountered {{ $value }} checkpoint creation failures
            summary: Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) >
            0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbCheckpointDeletionFailures
          annotations:
            description: Prometheus encountered {{ $value }} checkpoint deletion failures
            summary: Prometheus TSDB checkpoint deletion failures (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) >
            0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbCompactionsFailed
          annotations:
            description: Prometheus encountered {{ $value }} TSDB compactions failures
            summary: Prometheus TSDB compactions failed (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_compactions_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbHeadTruncationsFailed
          annotations:
            description: Prometheus encountered {{ $value }} TSDB head truncation
              failures
            summary: Prometheus TSDB head truncations failed (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbReloadFailures
          annotations:
            description: Prometheus encountered {{ $value }} TSDB reload failures
            summary: Prometheus TSDB reload failures (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_reloads_failures_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbWalCorruptions
          annotations:
            description: Prometheus encountered {{ $value }} TSDB WAL corruptions
            summary: Prometheus TSDB WAL corruptions (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbWalTruncationsFailed
          annotations:
            description: Prometheus encountered {{ $value }} TSDB WAL truncation failures
            summary: Prometheus TSDB WAL truncations failed (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
      - name: Redis
        rules:
        - alert: RedisDown
          annotations:
            description: Redis instance is down
            summary: Redis down (instance {{ $labels.instance }})
          expr: redis_up == 0
          for: 0m
          labels:
            severity: critical
        - alert: RedisMissingMaster
          annotations:
            description: Redis cluster has no node marked as master.
            summary: Redis missing master (instance {{ $labels.instance }})
          expr: (count(redis_instance_info{role="master"}) or vector(0)) < 1
          for: 0m
          labels:
            severity: critical
        - alert: RedisTooManyMasters
          annotations:
            description: Redis cluster has too many nodes marked as master.
            summary: Redis too many masters (instance {{ $labels.instance }})
          expr: count(redis_instance_info{role="master"}) > 1
          for: 0m
          labels:
            severity: critical
        - alert: RedisDisconnectedSlaves
          annotations:
            description: Redis not replicating for all slaves. Consider reviewing
              the redis replication status.
            summary: Redis disconnected slaves (instance {{ $labels.instance }})
          expr: count without (instance, job) (redis_connected_slaves) - sum without
            (instance, job) (redis_connected_slaves) - 1 > 1
          for: 0m
          labels:
            severity: critical
        - alert: RedisReplicationBroken
          annotations:
            description: Redis instance lost a slave
            summary: Redis replication broken (instance {{ $labels.instance }})
          expr: delta(redis_connected_slaves[1m]) < 0
          for: 0m
          labels:
            severity: critical
        - alert: RedisClusterFlapping
          annotations:
            description: Changes have been detected in Redis replica connection. This
              can occur when replica nodes lose connection to the master and reconnect
              (a.k.a flapping).
            summary: Redis cluster flapping (instance {{ $labels.instance }})
          expr: changes(redis_connected_slaves[1m]) > 1
          for: 2m
          labels:
            severity: critical
        - alert: RedisMissingBackup
          annotations:
            description: Redis has not been backuped for 24 hours
            summary: Redis missing backup (instance {{ $labels.instance }})
          expr: time() - redis_rdb_last_save_timestamp_seconds > 60 * 60 * 24
          for: 0m
          labels:
            severity: critical
        - alert: RedisOutOfSystemMemory
          annotations:
            description: Redis is running out of system memory (> 90%)
            summary: Redis out of system memory (instance {{ $labels.instance }})
          expr: redis_memory_used_bytes / redis_total_system_memory_bytes * 100 >
            90
          for: 2m
          labels:
            severity: warning
        - alert: RedisOutOfConfiguredMaxmemory
          annotations:
            description: Redis is running out of configured maxmemory (> 90%)
            summary: Redis out of configured maxmemory (instance {{ $labels.instance
              }})
          expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 90
          for: 2m
          labels:
            severity: warning
        - alert: RedisTooManyConnections
          annotations:
            description: Redis instance has too many connections
            summary: Redis too many connections (instance {{ $labels.instance }})
          expr: redis_connected_clients > 100
          for: 2m
          labels:
            severity: warning
        - alert: RedisNotEnoughConnections
          annotations:
            description: Redis instance should have more connections (> 5)
            summary: Redis not enough connections (instance {{ $labels.instance }})
          expr: redis_connected_clients < 5
          for: 2m
          labels:
            severity: warning
        - alert: RedisRejectedConnections
          annotations:
            description: Some connections to Redis has been rejected
            summary: Redis rejected connections (instance {{ $labels.instance }})
          expr: increase(redis_rejected_connections_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
prometheus-kafka-exporter:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "9308"
    prometheus.io/scrape: "true"
  enabled: false
  kafkaServer:
  - analytickit-analytickit-kafka:9092
prometheus-postgres-exporter:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "9187"
    prometheus.io/scrape: "true"
  config:
    datasource:
      host: analytickit-analytickit-postgresql
      passwordSecret:
        key: postgresql-password
        name: analytickit-analytickit-postgresql
      user: postgres
  enabled: false
prometheus-redis-exporter:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "9121"
    prometheus.io/scrape: "true"
  enabled: false
  redisAddress: redis://analytickit-analytickit-redis-master:6379
prometheus-statsd-exporter:
  enabled: false
  podAnnotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "9102"
    prometheus.io/scrape: "true"
promtail:
  config:
    clients:
    - url: http://analytickit-loki-write:3100/loki/api/v1/push
    snippets:
      pipelineStages:
      - cri: {}
      - match:
          selector: '{app="ingress-nginx"}'
          stages:
          - json:
              expressions:
                correlation_id: correlation_id
                forwarded_for: http_x_forwarded_for
                host: host
                method: method
                status: status
                timestamp: time
                uri: uri
                user_agent: http_user_agent
          - labels:
              method: null
              status: null
          - timestamp:
              format: RFC3339
              source: timestamp
      - match:
          selector: '{app="analytickit", container=~"analytickit-web|analytickit-worker|analytickit-events"}'
          stages:
          - json:
              expressions:
                level: null
                timestamp: null
          - labels:
              level: null
          - timestamp:
              format: RFC3339Nano
              source: timestamp
  enabled: false
  podAnnotations: {}
redis:
  architecture: standalone
  auth:
    enabled: false
    existingSecret: ""
    existingSecretPasswordKey: ""
    password: ""
  enabled: true
  fullnameOverride: ""
  master:
    extraFlags:
    - --maxmemory 400mb
    - --maxmemory-policy allkeys-lru
    persistence:
      enabled: true
      size: 5Gi
  nameOverride: analytickit-redis
saml:
  acs_url: null
  attr_email: null
  attr_first_name: null
  attr_last_name: null
  attr_permanent_id: null
  disabled: false
  enforced: false
  entity_id: null
  x509_cert: null
sentryDSN: null
service:
  annotations: {}
  externalPort: 8000
  internalPort: 8000
  name: analytickit
  type: NodePort
serviceAccount:
  annotations: {}
  create: true
  name: null
siteUrl: dpa.analytickit.com
web:
  affinity: {}
  enabled: true
  env:
  - name: SOCIAL_AUTH_GOOGLE_OAUTH2_KEY
    value: null
  - name: SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET
    value: null
  - name: SOCIAL_AUTH_GOOGLE_OAUTH2_WHITELISTED_DOMAINS
    value: analytickit.com
  hpa:
    behavior: null
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  internalMetrics:
    capture: true
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 0
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podAnnotations: null
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 0
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  secureCookies: true
  securityContext:
    enabled: false
  startupProbe:
    failureThreshold: 6
    initialDelaySeconds: 0
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 5
  tolerations: []
worker:
  affinity: {}
  enabled: true
  env: []
  hpa:
    behavior: null
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  tolerations: []
zookeeper:
  autopurge:
    purgeInterval: 1
  enabled: true
  metrics:
    enabled: false
    service:
      annotations:
        prometheus.io/scrape: "false"
  nameOverride: analytickit-zookeeper
  podAnnotations: null
  replicaCount: 1

COMPUTED VALUES:
busybox:
  image: busybox:1.34
  pullPolicy: IfNotPresent
cert-manager:
  enabled: false
  installCRDs: true
  podDnsConfig:
    nameservers:
    - 8.8.8.8
    - 1.1.1.1
    - 208.67.222.222
  podDnsPolicy: None
clickhouse:
  affinity: {}
  allowedNetworkIps:
  - 10.0.0.0/8
  - 172.16.0.0/12
  - 192.168.0.0/16
  backup:
    backup_password: backup_password
    backup_schedule: 0 0 * * *
    backup_user: backup
    clickhouse_services: chi-analytickit-analytickit-0-0
    enabled: false
    env:
    - name: LOG_LEVEL
      value: debug
    - name: ALLOW_EMPTY_BACKUPS
      value: "true"
    - name: API_LISTEN
      value: 0.0.0.0:7171
    - name: API_CREATE_INTEGRATION_TABLES
      value: "true"
    - name: BACKUPS_TO_KEEP_REMOTE
      value: "0"
    existingSecret: ""
    existingSecretPasswordKey: ""
    image:
      pullPolicy: IfNotPresent
      repository: altinity/clickhouse-backup
      tag: 1.5.0
  client:
    image:
      pullPolicy: IfNotPresent
      repository: clickhouse/clickhouse-server
      tag: 22.3.13.80
  cluster: analytickit
  database: analytickit
  defaultProfiles:
    default/allow_experimental_window_functions: "1"
    default/allow_nondeterministic_mutations: "1"
  defaultSettings:
    default_database: analytickit
    format_schema_path: /etc/clickhouse-server/config.d/
  enabled: true
  existingSecret: ""
  existingSecretPasswordKey: ""
  image:
    pullPolicy: IfNotPresent
    repository: clickhouse/clickhouse-server
    tag: 22.3.13.80
  layout:
    replicasCount: 1
    shardsCount: 1
  password: a1f31e03-c88e-4ca6-a2df-ad49183d15d9
  persistence:
    enabled: true
    existingClaim: ""
    size: 20Gi
  profiles: {}
  resources: {}
  secure: false
  securityContext:
    enabled: true
    fsGroup: 101
    runAsGroup: 101
    runAsUser: 101
  serviceType: ClusterIP
  settings: {}
  tolerations: []
  user: admin
  verify: false
cloud: aws
cloudwatch:
  enabled: false
  fluentBit:
    port: 2020
    readHead: "On"
    readTail: "Off"
    server: "On"
email:
  existingSecret: ""
  existingSecretKey: ""
  use_tls: true
env: []
eventrouter:
  enabled: false
  image:
    pullPolicy: IfNotPresent
    repository: gcr.io/heptio-images/eventrouter
    tag: v0.3
  resources: {}
events:
  enabled: true
  hpa:
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  podSecurityContext:
    enabled: false
  replicacount: 1
  securityContext:
    enabled: false
externalClickhouse:
  database: analytickit
  secure: false
  verify: false
externalKafka:
  brokers: []
externalObjectStorage: {}
externalPostgresql:
  existingSecretPasswordKey: postgresql-password
  postgresqlPort: 5432
externalRedis:
  existingSecret: ""
  existingSecretPasswordKey: ""
  host: ""
  password: ""
  port: 6379
externalStatsd: {}
grafana:
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - access: proxy
        isDefault: true
        jsonData:
          timeInterval: 60s
        name: Prometheus
        type: prometheus
        url: http://analytickit-prometheus-server
      - access: proxy
        isDefault: false
        name: Loki
        type: loki
        url: http://analytickit-loki-read:3100
      - access: proxy
        isDefault: false
        jsonData:
          implementation: prometheus
        name: Alertmanager
        type: alertmanager
        url: http://analytickit-prometheus-alertmanager
  enabled: false
  sidecar:
    dashboards:
      enabled: true
      folderAnnotation: grafana_folder
      label: grafana_dashboard
      provider:
        foldersFromFilesStructure: true
hooks:
  affinity: {}
  migrate:
    env: []
    resources: {}
  nodeSelector: {}
  tolerations: []
image:
  default: :release-1.41.3
  pullPolicy: IfNotPresent
  repository: analytickit/analytickit
ingress:
  annotations: {}
  enabled: true
  gcp:
    forceHttps: true
    ip_name: analytickit
    secretName: ""
  nginx:
    enabled: false
    redirectToTLS: true
ingress-nginx:
  controller:
    config:
      log-format-escape-json: "true"
      log-format-upstream: '{ "time": "$time_iso8601", "remote_addr": "$proxy_protocol_addr",
        "request_id": "$request_id", "correlation_id": "$request_id", "remote_user":
        "$remote_user", "bytes_sent": $bytes_sent, "request_time": $request_time,
        "status": $status, "host": "$host", "request_proto": "$server_protocol", "uri":
        "$uri", "request_query": "$args", "request_length": $request_length, "duration":
        $request_time, "method": "$request_method", "http_referrer": "$http_referer",
        "http_user_agent": "$http_user_agent", "http_x_forwarded_for": "$http_x_forwarded_for"
        }'
      use-forwarded-headers: "true"
    proxySetHeaders:
      X-Correlation-ID: $request_id
installCustomStorageClass: false
kafka:
  advertisedListeners: []
  affinity: {}
  allowEveryoneIfNoAclFound: true
  allowPlaintextListener: true
  args: []
  auth:
    clientProtocol: plaintext
    interBrokerProtocol: plaintext
    jaas:
      clientPasswords: []
      clientUsers:
      - user
      existingSecret: ""
      interBrokerPassword: ""
      interBrokerUser: admin
      zookeeperPassword: ""
      zookeeperUser: ""
    jksKeystoreSAN: ""
    jksPassword: ""
    jksSecret: ""
    jksTruststore: ""
    jksTruststoreSecret: ""
    sasl:
      interBrokerMechanism: plain
      jaas:
        clientPasswords: []
        clientUsers:
        - user
        existingSecret: ""
        interBrokerPassword: ""
        interBrokerUser: admin
        zookeeperPassword: ""
        zookeeperUser: ""
      mechanisms: plain,scram-sha-256,scram-sha-512
    saslInterBrokerMechanism: plain
    saslMechanisms: plain,scram-sha-256,scram-sha-512
    tls:
      autoGenerated: false
      endpointIdentificationAlgorithm: https
      existingSecret: ""
      existingSecrets: []
      jksKeystoreSAN: ""
      jksTruststore: ""
      jksTruststoreSecret: ""
      password: ""
      type: jks
    tlsEndpointIdentificationAlgorithm: https
  authorizerClassName: ""
  autoCreateTopicsEnable: true
  clusterDomain: cluster.local
  command:
  - /scripts/setup.sh
  common:
    exampleValue: common-chart
    global:
      imagePullSecrets: []
      imageRegistry: ""
      storageClass: ""
  commonAnnotations: {}
  commonLabels: {}
  config: ""
  containerSecurityContext:
    enabled: false
  customLivenessProbe: {}
  customReadinessProbe: {}
  defaultReplicationFactor: 1
  deleteTopicEnable: false
  diagnosticMode:
    args:
    - infinity
    command:
    - sleep
    enabled: false
  enabled: true
  existingConfigmap: ""
  existingLog4jConfigMap: ""
  externalAccess:
    autoDiscovery:
      enabled: false
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/kubectl
        tag: 1.23.1-debian-10-r26
      resources:
        limits: {}
        requests: {}
    enabled: false
    service:
      annotations: {}
      domain: ""
      loadBalancerIPs: []
      loadBalancerSourceRanges: []
      nodePorts: []
      port: 9094
      type: LoadBalancer
      useHostIPs: false
      usePodIPs: false
  externalZookeeper:
    servers:
    - analytickit-analytickit-zookeeper:2181
  extraDeploy: []
  extraEnvVars: []
  extraVolumeMounts: []
  extraVolumes: []
  fullnameOverride: ""
  global:
    imagePullSecrets: []
    imageRegistry: ""
    storageClass: ""
  heapOpts: -Xmx1024m -Xms1024m
  hostAliases: []
  image:
    debug: false
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: bitnami/kafka
    tag: 2.8.1-debian-10-r99
  initContainers: []
  interBrokerListenerName: INTERNAL
  listenerSecurityProtocolMap: ""
  listeners: []
  livenessProbe:
    enabled: true
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 5
  log4j: ""
  logFlushIntervalMessages: _10000
  logFlushIntervalMs: 1000
  logPersistence:
    accessModes:
    - ReadWriteOnce
    annotations: {}
    enabled: false
    existingClaim: ""
    existingLogClaim: ""
    mountPath: /opt/bitnami/kafka/logs
    selector: {}
    size: 8Gi
  logRetentionBytes: _15_000_000_000
  logRetentionCheckIntervalMs: 300000
  logRetentionHours: 24
  logSegmentBytes: _1073741824
  logsDirs: /bitnami/kafka/data
  maxMessageBytes: _1000012
  metrics:
    jmx:
      config: |-
        jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi
        lowercaseOutputName: true
        lowercaseOutputLabelNames: true
        ssl: false
        {{- if .Values.metrics.jmx.whitelistObjectNames }}
        whitelistObjectNames: ["{{ join "\",\"" .Values.metrics.jmx.whitelistObjectNames }}"]
        {{- end }}
      containerSecurityContext:
        enabled: false
      enabled: false
      existingConfigmap: ""
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/jmx-exporter
        tag: 0.16.1-debian-10-r177
      resources:
        limits: {}
        requests: {}
      service:
        annotations:
          prometheus.io/path: /
          prometheus.io/port: '{{ .Values.metrics.jmx.service.port }}'
          prometheus.io/scrape: "true"
        clusterIP: ""
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        nodePort: ""
        port: 5556
        type: ClusterIP
      whitelistObjectNames:
      - kafka.controller:*
      - kafka.server:*
      - java.lang:*
      - kafka.network:*
      - kafka.log:*
    kafka:
      affinity: {}
      certificatesSecret: ""
      containerSecurityContext:
        enabled: false
      enabled: false
      extraFlags: {}
      image:
        pullPolicy: IfNotPresent
        pullSecrets: []
        registry: docker.io
        repository: bitnami/kafka-exporter
        tag: 1.4.2-debian-10-r115
      initContainers: []
      nodeSelector: {}
      podAnnotations: {}
      podLabels: {}
      resources:
        limits: {}
        requests: {}
      schedulerName: ""
      service:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: '{{ .Values.metrics.kafka.service.port }}'
          prometheus.io/scrape: "true"
        clusterIP: ""
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        nodePort: ""
        port: 9308
        type: ClusterIP
      serviceAccount:
        automountServiceAccountToken: true
        create: true
        name: ""
      tlsCaCert: ca-file
      tlsCaSecret: ""
      tlsCert: cert-file
      tlsKey: key-file
      tolerations: []
    serviceMonitor:
      enabled: false
      interval: ""
      metricRelabelings: []
      namespace: ""
      relabelings: []
      scrapeTimeout: ""
      selector: {}
  minBrokerId: 0
  nameOverride: analytickit-kafka
  networkPolicy:
    allowExternal: true
    egressRules:
      customRules: []
    enabled: false
    explicitNamespacesSelector: {}
    externalAccess:
      from: []
  nodeAffinityPreset:
    key: ""
    type: ""
    values: []
  nodeSelector: {}
  numIoThreads: 8
  numNetworkThreads: 3
  numPartitions: 1
  numRecoveryThreadsPerDataDir: 1
  offsetsTopicReplicationFactor: 1
  pdb:
    create: false
    maxUnavailable: 1
    minAvailable: ""
  persistence:
    accessModes:
    - ReadWriteOnce
    annotations: {}
    enabled: true
    existingClaim: ""
    mountPath: /bitnami/kafka
    selector: {}
    size: 20Gi
    storageClass: ""
  podAffinityPreset: ""
  podAnnotations: {}
  podAntiAffinityPreset: soft
  podLabels: {}
  podManagementPolicy: Parallel
  podSecurityContext:
    enabled: true
    fsGroup: 1001
    runAsUser: 1001
  priorityClassName: ""
  provisioning:
    args: []
    command: []
    enabled: false
    numPartitions: 1
    podAnnotations: {}
    replicationFactor: 1
    resources:
      limits: {}
      requests: {}
    schedulerName: ""
    topics: []
  rbac:
    create: false
  readinessProbe:
    enabled: true
    failureThreshold: 6
    initialDelaySeconds: 5
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 5
  replicaCount: 1
  resources:
    limits: {}
    requests: {}
  rollingUpdatePartition: ""
  schedulerName: ""
  service:
    annotations: {}
    externalPort: 9094
    internalPort: 9093
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    nodePorts:
      client: ""
      external: ""
    port: 9092
    type: ClusterIP
  serviceAccount:
    automountServiceAccountToken: true
    create: true
    name: ""
  sidecars: []
  socketReceiveBufferBytes: 102400
  socketRequestMaxBytes: _104857600
  socketSendBufferBytes: 102400
  superUsers: User:admin
  terminationGracePeriodSeconds: ""
  tolerations: []
  topologySpreadConstraints: {}
  transactionStateLogMinIsr: 1
  transactionStateLogReplicationFactor: 1
  updateStrategy: RollingUpdate
  volumePermissions:
    enabled: false
    image:
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/bitnami-shell
      tag: 10-debian-10-r307
    resources:
      limits: {}
      requests: {}
    securityContext:
      runAsUser: 0
  zookeeper:
    auth:
      clientPassword: ""
      clientUser: ""
      enabled: false
      existingSecret: ""
      serverPasswords: ""
      serverUsers: ""
    enabled: false
  zookeeperChrootPath: ""
  zookeeperConnectionTimeoutMs: 6000
loki:
  enabled: false
  fullnameOverride: analytickit-loki
  gateway:
    enabled: false
  loki:
    auth_enabled: false
    commonConfig:
      replication_factor: 1
  minio:
    drivesPerNode: 2
    enabled: true
    replicas: 1
  monitoring:
    alerts:
      enabled: false
    dashboards:
      enabled: false
    rules:
      enabled: false
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
    serviceMonitor:
      enabled: false
  nameOverride: analytickit-loki
  read:
    replicas: 1
  write:
    replicas: 1
migrate:
  enabled: true
minio:
  auth:
    rootPassword: root-password-change-me-please
    rootUser: root-user
  defaultBuckets: analytickit
  disableWebUI: true
  enabled: false
  persistence:
    enabled: true
  service:
    ports:
      api: "19000"
      console: "19001"
pgbouncer:
  affinity: {}
  enabled: true
  env:
  - name: PGBOUNCER_PORT
    value: "6543"
  - name: PGBOUNCER_MAX_CLIENT_CONN
    value: "1000"
  - name: PGBOUNCER_POOL_MODE
    value: transaction
  - name: PGBOUNCER_IGNORE_STARTUP_PARAMETERS
    value: extra_float_digits
  exporter:
    enabled: false
    image:
      pullPolicy: IfNotPresent
      repository: prometheuscommunity/pgbouncer-exporter
      tag: v0.4.1
    port: 9127
    resources: {}
    securityContext:
      enabled: false
  hpa:
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  image:
    pullPolicy: IfNotPresent
    repository: bitnami/pgbouncer
    tag: 1.17.0
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 60
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podAnnotations: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 2
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  service:
    annotations: {}
    type: ClusterIP
  tolerations: []
plugins:
  affinity: {}
  enabled: true
  env: []
  hpa:
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  tolerations: []
pluginsAsync:
  affinity: {}
  enabled: false
  env: []
  hpa:
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  tolerations: []
pluginsExports:
  affinity: {}
  enabled: false
  env: []
  hpa:
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  tolerations: []
pluginsIngestion:
  affinity: {}
  enabled: false
  env: []
  hpa:
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  tolerations: []
pluginsJobs:
  affinity: {}
  enabled: false
  env: []
  hpa:
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  tolerations: []
pluginsScheduler:
  affinity: {}
  enabled: false
  env: []
  hpa:
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 50
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  tolerations: []
postgresql:
  enabled: true
  extraEnv: []
  global:
    postgresql: {}
  image:
    debug: false
    pullPolicy: IfNotPresent
    registry: docker.io
    repository: bitnami/postgresql
    tag: 11.7.0-debian-10-r9
  ldap:
    baseDN: ""
    bindDN: ""
    enabled: false
    port: ""
    prefix: ""
    scheme: ""
    search_attr: ""
    search_filter: ""
    server: ""
    suffix: ""
    tls: false
    url: ""
  livenessProbe:
    enabled: true
    failureThreshold: 6
    initialDelaySeconds: 30
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 5
  master:
    affinity: {}
    annotations: {}
    extraInitContainers: |
      # - name: do-something
      #   image: busybox
      #   command: ['do', 'something']
    extraVolumeMounts: []
    extraVolumes: []
    labels: {}
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: ""
    sidecars: []
    tolerations: []
  metrics:
    enabled: false
    image:
      pullPolicy: IfNotPresent
      registry: docker.io
      repository: bitnami/postgres-exporter
      tag: 0.8.0-debian-10-r28
    livenessProbe:
      enabled: true
      failureThreshold: 6
      initialDelaySeconds: 5
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    prometheusRule:
      additionalLabels: {}
      enabled: false
      namespace: ""
      rules: []
    readinessProbe:
      enabled: true
      failureThreshold: 6
      initialDelaySeconds: 5
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    securityContext:
      enabled: false
      runAsUser: 1001
    service:
      annotations:
        prometheus.io/port: "9187"
        prometheus.io/scrape: "true"
      type: ClusterIP
    serviceMonitor:
      additionalLabels: {}
      enabled: false
  nameOverride: analytickit-postgresql
  networkPolicy:
    allowExternal: true
    enabled: false
  persistence:
    accessModes:
    - ReadWriteOnce
    annotations: {}
    enabled: true
    mountPath: /bitnami/postgresql
    size: 10Gi
    subPath: ""
  postgresqlDataDir: /bitnami/postgresql/data
  postgresqlDatabase: analytickit
  postgresqlPassword: postgres
  postgresqlUsername: postgres
  readinessProbe:
    enabled: true
    failureThreshold: 6
    initialDelaySeconds: 5
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 5
  replication:
    applicationName: my_application
    enabled: false
    numSynchronousReplicas: 0
    password: repl_password
    slaveReplicas: 1
    synchronousCommit: "off"
    user: repl_user
  resources:
    requests:
      cpu: 250m
      memory: 256Mi
  securityContext:
    enabled: true
    fsGroup: 1001
    runAsUser: 1001
  service:
    annotations: {}
    port: 5432
    type: ClusterIP
  serviceAccount:
    enabled: false
  shmVolume:
    chmod:
      enabled: true
    enabled: true
  slave:
    affinity: {}
    annotations: {}
    extraInitContainers: |
      # - name: do-something
      #   image: busybox
      #   command: ['do', 'something']
    extraVolumeMounts: []
    extraVolumes: []
    labels: {}
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: ""
    sidecars: []
    tolerations: []
  updateStrategy:
    type: RollingUpdate
  volumePermissions:
    enabled: false
    image:
      pullPolicy: Always
      registry: docker.io
      repository: bitnami/minideb
      tag: buster
    securityContext:
      runAsUser: 0
posthogSecretKey:
  existingSecretKey: analytickit-secret
prometheus:
  alertmanager:
    enabled: false
    podAnnotations: {}
  enabled: false
  pushgateway:
    enabled: false
  serverFiles:
    alerting_rules.yml:
      groups:
      - name: Kubernetes
        rules:
        - alert: KubernetesNodeReady
          annotations:
            description: Node {{ $labels.node }} has been unready for a long time
            summary: Kubernetes Node ready (instance {{ $labels.instance }})
          expr: kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 10m
          labels:
            severity: critical
        - alert: KubernetesMemoryPressure
          annotations:
            description: '{{ $labels.node }} has MemoryPressure condition'
            summary: Kubernetes memory pressure (instance {{ $labels.instance }})
          expr: kube_node_status_condition{condition="MemoryPressure",status="true"}
            == 1
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesDiskPressure
          annotations:
            description: '{{ $labels.node }} has DiskPressure condition'
            summary: Kubernetes disk pressure (instance {{ $labels.instance }})
          expr: kube_node_status_condition{condition="DiskPressure",status="true"}
            == 1
          for: 10m
          labels:
            severity: critical
        - alert: KubernetesOutOfDisk
          annotations:
            description: '{{ $labels.node }} has OutOfDisk condition'
            summary: Kubernetes out of disk (instance {{ $labels.instance }})
          expr: kube_node_status_condition{condition="OutOfDisk",status="true"} ==
            1
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesOutOfCapacity
          annotations:
            description: '{{ $labels.node }} is out of capacity'
            summary: Kubernetes out of capacity (instance {{ $labels.instance }})
          expr: sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid)
            group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node)
            (kube_node_status_allocatable{resource="pods"}) * 100 > 90
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesContainerOomKiller
          annotations:
            description: Container {{ $labels.container }} in pod {{ $labels.namespace
              }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last
              10 minutes.
            summary: Kubernetes container oom killer (instance {{ $labels.instance
              }})
          expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total
            offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m])
            == 1
          for: 0m
          labels:
            severity: critical
        - alert: KubernetesJobFailed
          annotations:
            description: Job {{$labels.namespace}}/{{$labels.exported_job}} failed
              to complete
            summary: Kubernetes Job failed (instance {{ $labels.instance }})
          expr: kube_job_status_failed > 0
          for: 0m
          labels:
            severity: warning
        - alert: KubernetesCronjobSuspended
          annotations:
            description: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is
              suspended
            summary: Kubernetes CronJob suspended (instance {{ $labels.instance }})
          expr: kube_cronjob_spec_suspend != 0
          for: 0m
          labels:
            severity: warning
        - alert: KubernetesPersistentvolumeclaimPending
          annotations:
            description: PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim
              }} is pending
            summary: Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance
              }})
          expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
          for: 2m
          labels:
            severity: warning
        - alert: KubernetesVolumeOutOfDiskSpace
          annotations:
            description: Volume is almost full (< 10% left)
            summary: Kubernetes Volume out of disk space (instance {{ $labels.instance
              }})
          expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes
            * 100 < 10
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesVolumeFullInFourDays
          annotations:
            description: '{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim
              }} is expected to fill up within four days. Currently {{ $value | humanize
              }}% is available.'
            summary: Kubernetes Volume full in four days (instance {{ $labels.instance
              }})
          expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 *
            3600) < 0
          for: 0m
          labels:
            severity: critical
        - alert: KubernetesPersistentvolumeError
          annotations:
            description: Persistent volume is in bad state
            summary: Kubernetes PersistentVolume error (instance {{ $labels.instance
              }})
          expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"}
            > 0
          for: 0m
          labels:
            severity: critical
        - alert: KubernetesStatefulsetDown
          annotations:
            description: A StatefulSet went down
            summary: Kubernetes StatefulSet down (instance {{ $labels.instance }})
          expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current)
            != 1
          for: 1m
          labels:
            severity: critical
        - alert: KubernetesHpaScalingAbility
          annotations:
            description: Pod is unable to scale
            summary: Kubernetes HPA scaling ability (instance {{ $labels.instance
              }})
          expr: kube_horizontalpodautoscaler_status_condition{status="false", condition="AbleToScale"}
            == 1
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesHpaMetricAvailability
          annotations:
            description: HPA is not able to collect metrics
            summary: Kubernetes HPA metric availability (instance {{ $labels.instance
              }})
          expr: kube_horizontalpodautoscaler_status_condition{status="false", condition="ScalingActive"}
            == 1
          for: 5m
          labels:
            severity: critical
        - alert: KubernetesHpaScaleCapability
          annotations:
            description: The maximum number of desired Pods has been hit
            summary: Kubernetes HPA scale capability (instance {{ $labels.instance
              }})
          expr: kube_horizontalpodautoscaler_status_desired_replicas >= kube_horizontalpodautoscaler_spec_max_replicas
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesPodNotHealthy
          annotations:
            description: Pod has been in a non-ready state for longer than 15 minutes.
            summary: Kubernetes Pod not healthy (instance {{ $labels.instance }})
          expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})
            > 0
          for: 15m
          labels:
            severity: warning
        - alert: KubernetesPodCrashLooping
          annotations:
            description: Pod {{ $labels.pod }} is crash looping
            summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
          expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
          for: 2m
          labels:
            severity: warning
        - alert: KubernetesReplicassetMismatch
          annotations:
            description: |
              The number of ready pods in the Deployment's replicaset does not match the desired number.
            summary: Kubernetes ReplicasSet mismatch (instance {{ $labels.instance
              }})
          expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
          for: 10m
          labels:
            severity: warning
        - alert: KubernetesDeploymentReplicasMismatch
          annotations:
            description: |
              The number of ready pods in the Deployment does not match the desired number.
            summary: Kubernetes Deployment replicas mismatch (instance {{ $labels.instance
              }})
          expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
          for: 10m
          labels:
            severity: warning
        - alert: KubernetesStatefulsetReplicasMismatch
          annotations:
            description: |
              The number of ready pods in the StatefulSet does not match the desired number.
            summary: Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance
              }})
          expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
          for: 10m
          labels:
            severity: warning
        - alert: KubernetesDeploymentGenerationMismatch
          annotations:
            description: A Deployment has failed but has not been rolled back.
            summary: Kubernetes Deployment generation mismatch (instance {{ $labels.instance
              }})
          expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
          for: 10m
          labels:
            severity: critical
        - alert: KubernetesStatefulsetGenerationMismatch
          annotations:
            description: A StatefulSet has failed but has not been rolled back.
            summary: Kubernetes StatefulSet generation mismatch (instance {{ $labels.instance
              }})
          expr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
          for: 10m
          labels:
            severity: critical
        - alert: KubernetesStatefulsetUpdateNotRolledOut
          annotations:
            description: StatefulSet update has not been rolled out.
            summary: Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance
              }})
          expr: max without (revision) (kube_statefulset_status_current_revision unless
            kube_statefulset_status_update_revision) * (kube_statefulset_replicas
            != kube_statefulset_status_replicas_updated)
          for: 10m
          labels:
            severity: warning
        - alert: KubernetesDaemonsetRolloutStuck
          annotations:
            description: Some Pods of DaemonSet are not scheduled or not ready
            summary: Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance
              }})
          expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled
            * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled
            > 0
          for: 10m
          labels:
            severity: warning
        - alert: KubernetesDaemonsetMisscheduled
          annotations:
            description: Some DaemonSet Pods are running where they are not supposed
              to run
            summary: Kubernetes DaemonSet misscheduled (instance {{ $labels.instance
              }})
          expr: kube_daemonset_status_number_misscheduled > 0
          for: 5m
          labels:
            severity: critical
        - alert: KubernetesCronjobTooLong
          annotations:
            description: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is
              taking more than 1h to complete.
            summary: Kubernetes CronJob too long (instance {{ $labels.instance }})
          expr: time() - kube_cronjob_next_schedule_time > 3600
          for: 0m
          labels:
            severity: warning
        - alert: KubernetesJobSlowCompletion
          annotations:
            description: Kubernetes Job {{ $labels.namespace }}/{{ $labels.job_name
              }} did not complete in time.
            summary: Kubernetes job slow completion (instance {{ $labels.instance
              }})
          expr: kube_job_spec_completions - kube_job_status_succeeded > 0
          for: 12h
          labels:
            severity: critical
        - alert: KubernetesApiServerErrors
          annotations:
            description: Kubernetes API server is experiencing high error rate
            summary: Kubernetes API server errors (instance {{ $labels.instance }})
          expr: sum(rate(apiserver_request_total{job="apiserver",code=~"^(?:5..)$"}[1m]))
            / sum(rate(apiserver_request_total{job="apiserver"}[1m])) * 100 > 3
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesApiClientErrors
          annotations:
            description: Kubernetes API client is experiencing high error rate
            summary: Kubernetes API client errors (instance {{ $labels.instance }})
          expr: (sum(rate(rest_client_requests_total{code=~"(4|5).."}[1m])) by (instance,
            job) / sum(rate(rest_client_requests_total[1m])) by (instance, job)) *
            100 > 1
          for: 2m
          labels:
            severity: critical
        - alert: KubernetesClientCertificateExpiresNextWeek
          annotations:
            description: A client certificate used to authenticate to the apiserver
              is expiring next week.
            summary: Kubernetes client certificate expires next week (instance {{
              $labels.instance }})
          expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"}
            > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m])))
            < 7*24*60*60
          for: 0m
          labels:
            severity: warning
        - alert: KubernetesClientCertificateExpiresSoon
          annotations:
            description: A client certificate used to authenticate to the apiserver
              is expiring in less than 24.0 hours.
            summary: Kubernetes client certificate expires soon (instance {{ $labels.instance
              }})
          expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"}
            > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m])))
            < 24*60*60
          for: 0m
          labels:
            severity: critical
        - alert: KubernetesApiServerLatency
          annotations:
            description: Kubernetes API server has a 99th percentile latency of {{
              $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.
            summary: Kubernetes API server latency (instance {{ $labels.instance }})
          expr: histogram_quantile(0.99, sum(rate(apiserver_request_latencies_bucket{subresource!="log",verb!~"^(?:CONNECT|WATCHLIST|WATCH|PROXY)$"}
            [10m])) WITHOUT (instance, resource)) / 1e+06 > 1
          for: 2m
          labels:
            severity: warning
      - name: Loki
        rules:
        - alert: LokiProcessTooManyRestarts
          annotations:
            description: A loki process had too many restarts (target {{ $labels.instance
              }})
            summary: Loki process too many restarts (instance {{ $labels.instance
              }})
          expr: changes(process_start_time_seconds{app="loki"}[15m]) > 2
          for: 0m
          labels:
            severity: warning
        - alert: LokiRequestErrors
          annotations:
            description: The {{ $labels.job }} and {{ $labels.route }} are experiencing
              errors
            summary: Loki request errors (instance {{ $labels.instance }})
          expr: 100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m]))
            by (namespace, job, route) / sum(rate(loki_request_duration_seconds_count[1m]))
            by (namespace, job, route) > 10
          for: 15m
          labels:
            severity: warning
        - alert: LokiRequestPanic
          annotations:
            description: The {{ $labels.job }} is experiencing {{ printf "%.2f" $value
              }}% increase of panics
            summary: Loki request panic (instance {{ $labels.instance }})
          expr: sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
          for: 5m
          labels:
            severity: warning
        - alert: LokiRequestLatency
          annotations:
            description: The {{ $labels.job }} {{ $labels.route }} is experiencing
              {{ printf "%.2f" $value }}s 99th percentile latency
            summary: Loki request latency (instance {{ $labels.instance }})
          expr: (histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[5m]))
            by (le))) > 3
          for: 10m
          labels:
            severity: warning
      - name: Promtail
        rules:
        - alert: PromtailRequestErrors
          annotations:
            description: The {{ $labels.job }} {{ $labels.route }} is experiencing
              {{ printf "%.2f" $value }}% errors.
            summary: Promtail request errors (instance {{ $labels.instance }})
          expr: 100 * sum(rate(promtail_request_duration_seconds_count{status_code=~"5..|failed"}[1m]))
            by (namespace, job, route, instance) / sum(rate(promtail_request_duration_seconds_count[1m]))
            by (namespace, job, route, instance) > 10
          for: 5m
          labels:
            severity: critical
        - alert: PromtailRequestLatency
          annotations:
            description: The {{ $labels.job }} {{ $labels.route }} is experiencing
              {{ printf "%.2f" $value }}s 99th percentile latency.
            summary: Promtail request latency (instance {{ $labels.instance }})
          expr: histogram_quantile(0.99, sum(rate(promtail_request_duration_seconds_bucket[5m]))
            by (le)) > 1
          for: 5m
          labels:
            severity: critical
      - name: Prometheus
        rules:
        - alert: PrometheusJobMissing
          annotations:
            description: A Prometheus job has disappeared
            summary: Prometheus job missing (instance {{ $labels.instance }})
          expr: absent(up{job="prometheus"})
          for: 0m
          labels:
            severity: warning
        - alert: PrometheusTargetMissing
          annotations:
            description: A Prometheus target has disappeared. An exporter might be
              crashed.
            summary: Prometheus target missing (instance {{ $labels.instance }})
          expr: up == 0
          for: 5m
          labels:
            severity: critical
        - alert: PrometheusAllTargetsMissing
          annotations:
            description: A Prometheus job does not have living target anymore.
            summary: Prometheus all targets missing (instance {{ $labels.instance
              }})
          expr: sum by (job) (up) == 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusConfigurationReloadFailure
          annotations:
            description: Prometheus configuration reload error
            summary: Prometheus configuration reload failure (instance {{ $labels.instance
              }})
          expr: prometheus_config_last_reload_successful != 1
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTooManyRestarts
          annotations:
            description: Prometheus has restarted more than twice in the last 15 minutes.
              It might be crashlooping.
            summary: Prometheus too many restarts (instance {{ $labels.instance }})
          expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m])
            > 2
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusAlertmanagerJobMissing
          annotations:
            description: A Prometheus AlertManager job has disappeared
            summary: Prometheus AlertManager job missing (instance {{ $labels.instance
              }})
          expr: absent(up{job="kubernetes-pods", app="prometheus", component="alertmanager"})
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusAlertmanagerConfigurationReloadFailure
          annotations:
            description: AlertManager configuration reload error
            summary: Prometheus AlertManager configuration reload failure (instance
              {{ $labels.instance }})
          expr: alertmanager_config_last_reload_successful != 1
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusAlertmanagerConfigNotSynced
          annotations:
            description: Configurations of AlertManager cluster instances are out
              of sync
            summary: Prometheus AlertManager config not synced (instance {{ $labels.instance
              }})
          expr: count(count_values("config_hash", alertmanager_config_hash)) > 1
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusAlertmanagerE2eDeadManSwitch
          annotations:
            description: Prometheus DeadManSwitch is an always-firing alert. It's
              used as an end-to-end test of Prometheus through the Alertmanager.
            summary: Prometheus AlertManager E2E dead man switch (instance {{ $labels.instance
              }})
          expr: vector(1)
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusNotConnectedToAlertmanager
          annotations:
            description: Prometheus cannot connect the alertmanager
            summary: Prometheus not connected to alertmanager (instance {{ $labels.instance
              }})
          expr: prometheus_notifications_alertmanagers_discovered < 1
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusRuleEvaluationFailures
          annotations:
            description: Prometheus encountered {{ $value }} rule evaluation failures,
              leading to potentially ignored alerts.
            summary: Prometheus rule evaluation failures (instance {{ $labels.instance
              }})
          expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTemplateTextExpansionFailures
          annotations:
            description: Prometheus encountered {{ $value }} template text expansion
              failures
            summary: Prometheus template text expansion failures (instance {{ $labels.instance
              }})
          expr: increase(prometheus_template_text_expansion_failures_total[3m]) >
            0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusRuleEvaluationSlow
          annotations:
            description: Prometheus rule evaluation took more time than the scheduled
              interval. It indicates a slower storage backend access or too complex
              query.
            summary: Prometheus rule evaluation slow (instance {{ $labels.instance
              }})
          expr: prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds
          for: 5m
          labels:
            severity: warning
        - alert: PrometheusNotificationsBacklog
          annotations:
            description: The Prometheus notification queue has not been empty for
              10 minutes
            summary: Prometheus notifications backlog (instance {{ $labels.instance
              }})
          expr: min_over_time(prometheus_notifications_queue_length[10m]) > 0
          for: 0m
          labels:
            severity: warning
        - alert: PrometheusAlertmanagerNotificationFailing
          annotations:
            description: Alertmanager is failing sending notifications
            summary: Prometheus AlertManager notification failing (instance {{ $labels.instance
              }})
          expr: rate(alertmanager_notifications_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTargetEmpty
          annotations:
            description: Prometheus has no target in service discovery
            summary: Prometheus target empty (instance {{ $labels.instance }})
          expr: prometheus_sd_discovered_targets == 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTargetScrapingSlow
          annotations:
            description: Prometheus is scraping exporters slowly since it exceeded
              the requested interval time. Your Prometheus server is under-provisioned.
            summary: Prometheus target scraping slow (instance {{ $labels.instance
              }})
          expr: prometheus_target_interval_length_seconds{quantile="0.9"} / on (interval,
            instance, job) prometheus_target_interval_length_seconds{quantile="0.5"}
            > 1.05
          for: 5m
          labels:
            severity: warning
        - alert: PrometheusLargeScrape
          annotations:
            description: Prometheus has many scrapes that exceed the sample limit
            summary: Prometheus large scrape (instance {{ $labels.instance }})
          expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m])
            > 10
          for: 5m
          labels:
            severity: warning
        - alert: PrometheusTargetScrapeDuplicate
          annotations:
            description: Prometheus has many samples rejected due to duplicate timestamps
              but different values
            summary: Prometheus target scrape duplicate (instance {{ $labels.instance
              }})
          expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m])
            > 0
          for: 0m
          labels:
            severity: warning
        - alert: PrometheusTsdbCheckpointCreationFailures
          annotations:
            description: Prometheus encountered {{ $value }} checkpoint creation failures
            summary: Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) >
            0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbCheckpointDeletionFailures
          annotations:
            description: Prometheus encountered {{ $value }} checkpoint deletion failures
            summary: Prometheus TSDB checkpoint deletion failures (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) >
            0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbCompactionsFailed
          annotations:
            description: Prometheus encountered {{ $value }} TSDB compactions failures
            summary: Prometheus TSDB compactions failed (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_compactions_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbHeadTruncationsFailed
          annotations:
            description: Prometheus encountered {{ $value }} TSDB head truncation
              failures
            summary: Prometheus TSDB head truncations failed (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbReloadFailures
          annotations:
            description: Prometheus encountered {{ $value }} TSDB reload failures
            summary: Prometheus TSDB reload failures (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_reloads_failures_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbWalCorruptions
          annotations:
            description: Prometheus encountered {{ $value }} TSDB WAL corruptions
            summary: Prometheus TSDB WAL corruptions (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
        - alert: PrometheusTsdbWalTruncationsFailed
          annotations:
            description: Prometheus encountered {{ $value }} TSDB WAL truncation failures
            summary: Prometheus TSDB WAL truncations failed (instance {{ $labels.instance
              }})
          expr: increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
      - name: Redis
        rules:
        - alert: RedisDown
          annotations:
            description: Redis instance is down
            summary: Redis down (instance {{ $labels.instance }})
          expr: redis_up == 0
          for: 0m
          labels:
            severity: critical
        - alert: RedisMissingMaster
          annotations:
            description: Redis cluster has no node marked as master.
            summary: Redis missing master (instance {{ $labels.instance }})
          expr: (count(redis_instance_info{role="master"}) or vector(0)) < 1
          for: 0m
          labels:
            severity: critical
        - alert: RedisTooManyMasters
          annotations:
            description: Redis cluster has too many nodes marked as master.
            summary: Redis too many masters (instance {{ $labels.instance }})
          expr: count(redis_instance_info{role="master"}) > 1
          for: 0m
          labels:
            severity: critical
        - alert: RedisDisconnectedSlaves
          annotations:
            description: Redis not replicating for all slaves. Consider reviewing
              the redis replication status.
            summary: Redis disconnected slaves (instance {{ $labels.instance }})
          expr: count without (instance, job) (redis_connected_slaves) - sum without
            (instance, job) (redis_connected_slaves) - 1 > 1
          for: 0m
          labels:
            severity: critical
        - alert: RedisReplicationBroken
          annotations:
            description: Redis instance lost a slave
            summary: Redis replication broken (instance {{ $labels.instance }})
          expr: delta(redis_connected_slaves[1m]) < 0
          for: 0m
          labels:
            severity: critical
        - alert: RedisClusterFlapping
          annotations:
            description: Changes have been detected in Redis replica connection. This
              can occur when replica nodes lose connection to the master and reconnect
              (a.k.a flapping).
            summary: Redis cluster flapping (instance {{ $labels.instance }})
          expr: changes(redis_connected_slaves[1m]) > 1
          for: 2m
          labels:
            severity: critical
        - alert: RedisMissingBackup
          annotations:
            description: Redis has not been backuped for 24 hours
            summary: Redis missing backup (instance {{ $labels.instance }})
          expr: time() - redis_rdb_last_save_timestamp_seconds > 60 * 60 * 24
          for: 0m
          labels:
            severity: critical
        - alert: RedisOutOfSystemMemory
          annotations:
            description: Redis is running out of system memory (> 90%)
            summary: Redis out of system memory (instance {{ $labels.instance }})
          expr: redis_memory_used_bytes / redis_total_system_memory_bytes * 100 >
            90
          for: 2m
          labels:
            severity: warning
        - alert: RedisOutOfConfiguredMaxmemory
          annotations:
            description: Redis is running out of configured maxmemory (> 90%)
            summary: Redis out of configured maxmemory (instance {{ $labels.instance
              }})
          expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 90
          for: 2m
          labels:
            severity: warning
        - alert: RedisTooManyConnections
          annotations:
            description: Redis instance has too many connections
            summary: Redis too many connections (instance {{ $labels.instance }})
          expr: redis_connected_clients > 100
          for: 2m
          labels:
            severity: warning
        - alert: RedisNotEnoughConnections
          annotations:
            description: Redis instance should have more connections (> 5)
            summary: Redis not enough connections (instance {{ $labels.instance }})
          expr: redis_connected_clients < 5
          for: 2m
          labels:
            severity: warning
        - alert: RedisRejectedConnections
          annotations:
            description: Some connections to Redis has been rejected
            summary: Redis rejected connections (instance {{ $labels.instance }})
          expr: increase(redis_rejected_connections_total[1m]) > 0
          for: 0m
          labels:
            severity: critical
prometheus-kafka-exporter:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "9308"
    prometheus.io/scrape: "true"
  enabled: false
  kafkaServer:
  - analytickit-analytickit-kafka:9092
prometheus-postgres-exporter:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "9187"
    prometheus.io/scrape: "true"
  config:
    datasource:
      host: analytickit-analytickit-postgresql
      passwordSecret:
        key: postgresql-password
        name: analytickit-analytickit-postgresql
      user: postgres
  enabled: false
prometheus-redis-exporter:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "9121"
    prometheus.io/scrape: "true"
  enabled: false
  redisAddress: redis://analytickit-analytickit-redis-master:6379
prometheus-statsd-exporter:
  enabled: false
  podAnnotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "9102"
    prometheus.io/scrape: "true"
promtail:
  config:
    clients:
    - url: http://analytickit-loki-write:3100/loki/api/v1/push
    snippets:
      pipelineStages:
      - cri: {}
      - match:
          selector: '{app="ingress-nginx"}'
          stages:
          - json:
              expressions:
                correlation_id: correlation_id
                forwarded_for: http_x_forwarded_for
                host: host
                method: method
                status: status
                timestamp: time
                uri: uri
                user_agent: http_user_agent
          - labels:
              method: null
              status: null
          - timestamp:
              format: RFC3339
              source: timestamp
      - match:
          selector: '{app="analytickit", container=~"analytickit-web|analytickit-worker|analytickit-events"}'
          stages:
          - json:
              expressions:
                level: null
                timestamp: null
          - labels:
              level: null
          - timestamp:
              format: RFC3339Nano
              source: timestamp
  enabled: false
  podAnnotations: {}
redis:
  architecture: standalone
  auth:
    enabled: false
    existingSecret: ""
    existingSecretPasswordKey: ""
    password: ""
    sentinel: true
    usePasswordFiles: false
  clusterDomain: cluster.local
  common:
    exampleValue: common-chart
    global:
      imagePullSecrets: []
      imageRegistry: ""
      redis:
        password: ""
      storageClass: ""
  commonAnnotations: {}
  commonConfiguration: |-
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
  commonLabels: {}
  diagnosticMode:
    args:
    - infinity
    command:
    - sleep
    enabled: false
  enabled: true
  existingConfigmap: ""
  extraDeploy: []
  fullnameOverride: ""
  global:
    imagePullSecrets: []
    imageRegistry: ""
    redis:
      password: ""
    storageClass: ""
  image:
    debug: false
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: bitnami/redis
    tag: 6.2.7-debian-10-r0
  kubeVersion: ""
  master:
    affinity: {}
    args: []
    command: []
    configuration: ""
    containerPorts:
      redis: 6379
    containerSecurityContext:
      enabled: true
      runAsUser: 1001
    customLivenessProbe: {}
    customReadinessProbe: {}
    customStartupProbe: {}
    disableCommands:
    - FLUSHDB
    - FLUSHALL
    dnsConfig: {}
    dnsPolicy: ""
    extraEnvVars: []
    extraEnvVarsCM: ""
    extraEnvVarsSecret: ""
    extraFlags:
    - --maxmemory 400mb
    - --maxmemory-policy allkeys-lru
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    initContainers: []
    kind: StatefulSet
    lifecycleHooks: {}
    livenessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 20
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 5
    nodeAffinityPreset:
      key: ""
      type: ""
      values: []
    nodeSelector: {}
    persistence:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      dataSource: {}
      enabled: true
      existingClaim: ""
      medium: ""
      path: /data
      selector: {}
      size: 5Gi
      sizeLimit: ""
      storageClass: ""
      subPath: ""
    podAffinityPreset: ""
    podAnnotations: {}
    podAntiAffinityPreset: soft
    podLabels: {}
    podSecurityContext:
      enabled: true
      fsGroup: 1001
    preExecCmds: []
    priorityClassName: ""
    readinessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 20
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      limits: {}
      requests: {}
    schedulerName: ""
    service:
      annotations: {}
      clusterIP: ""
      externalTrafficPolicy: Cluster
      extraPorts: []
      internalTrafficPolicy: Cluster
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      nodePorts:
        redis: ""
      ports:
        redis: 6379
      type: ClusterIP
    shareProcessNamespace: false
    sidecars: []
    startupProbe:
      enabled: false
      failureThreshold: 5
      initialDelaySeconds: 20
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 5
    terminationGracePeriodSeconds: 30
    tolerations: []
    topologySpreadConstraints: []
    updateStrategy:
      rollingUpdate: {}
      type: RollingUpdate
  metrics:
    command: []
    containerSecurityContext:
      enabled: true
      runAsUser: 1001
    enabled: false
    extraArgs: {}
    extraEnvVars: []
    extraVolumeMounts: []
    extraVolumes: []
    image:
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/redis-exporter
      tag: 1.37.0-debian-10-r39
    podAnnotations:
      prometheus.io/port: "9121"
      prometheus.io/scrape: "true"
    podLabels: {}
    prometheusRule:
      additionalLabels: {}
      enabled: false
      namespace: ""
      rules: []
    redisTargetHost: localhost
    resources:
      limits: {}
      requests: {}
    service:
      annotations: {}
      externalTrafficPolicy: Cluster
      extraPorts: []
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      port: 9121
      type: ClusterIP
    serviceMonitor:
      additionalLabels: {}
      enabled: false
      honorLabels: false
      interval: 30s
      metricRelabelings: []
      namespace: ""
      relabellings: []
      scrapeTimeout: ""
  nameOverride: analytickit-redis
  networkPolicy:
    allowExternal: true
    enabled: false
    extraEgress: []
    extraIngress: []
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  pdb:
    create: false
    maxUnavailable: ""
    minAvailable: 1
  podSecurityPolicy:
    create: false
    enabled: false
  rbac:
    create: false
    rules: []
  replica:
    affinity: {}
    args: []
    autoscaling:
      enabled: false
      maxReplicas: 11
      minReplicas: 1
      targetCPU: ""
      targetMemory: ""
    command: []
    configuration: ""
    containerPorts:
      redis: 6379
    containerSecurityContext:
      enabled: true
      runAsUser: 1001
    customLivenessProbe: {}
    customReadinessProbe: {}
    customStartupProbe: {}
    disableCommands:
    - FLUSHDB
    - FLUSHALL
    dnsConfig: {}
    dnsPolicy: ""
    externalMaster:
      enabled: false
      host: ""
      port: 6379
    extraEnvVars: []
    extraEnvVarsCM: ""
    extraEnvVarsSecret: ""
    extraFlags: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    initContainers: []
    lifecycleHooks: {}
    livenessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 20
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 5
    nodeAffinityPreset:
      key: ""
      type: ""
      values: []
    nodeSelector: {}
    persistence:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      dataSource: {}
      enabled: true
      medium: ""
      path: /data
      selector: {}
      size: 8Gi
      storageClass: ""
      subPath: ""
    podAffinityPreset: ""
    podAnnotations: {}
    podAntiAffinityPreset: soft
    podLabels: {}
    podManagementPolicy: ""
    podSecurityContext:
      enabled: true
      fsGroup: 1001
    preExecCmds: []
    priorityClassName: ""
    readinessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 20
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    replicaCount: 3
    resources:
      limits: {}
      requests: {}
    schedulerName: ""
    service:
      annotations: {}
      clusterIP: ""
      externalTrafficPolicy: Cluster
      extraPorts: []
      internalTrafficPolicy: Cluster
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      nodePorts:
        redis: ""
      ports:
        redis: 6379
      type: ClusterIP
    shareProcessNamespace: false
    sidecars: []
    startupProbe:
      enabled: true
      failureThreshold: 22
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    terminationGracePeriodSeconds: 30
    tolerations: []
    topologySpreadConstraints: []
    updateStrategy:
      rollingUpdate: {}
      type: RollingUpdate
  secretAnnotations: {}
  sentinel:
    args: []
    automateClusterRecovery: false
    command: []
    configuration: ""
    containerPorts:
      sentinel: 26379
    containerSecurityContext:
      enabled: true
      runAsUser: 1001
    customLivenessProbe: {}
    customReadinessProbe: {}
    customStartupProbe: {}
    downAfterMilliseconds: 60000
    enabled: false
    externalMaster:
      enabled: false
      host: ""
      port: 6379
    extraEnvVars: []
    extraEnvVarsCM: ""
    extraEnvVarsSecret: ""
    extraVolumeMounts: []
    extraVolumes: []
    failoverTimeout: 18000
    getMasterTimeout: 220
    image:
      debug: false
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/redis-sentinel
      tag: 6.2.7-debian-10-r0
    lifecycleHooks: {}
    livenessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 20
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 5
    masterSet: mymaster
    parallelSyncs: 1
    persistence:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      dataSource: {}
      enabled: false
      selector: {}
      size: 100Mi
      storageClass: ""
    preExecCmds: []
    quorum: 2
    readinessProbe:
      enabled: true
      failureThreshold: 5
      initialDelaySeconds: 20
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      limits: {}
      requests: {}
    service:
      annotations: {}
      clusterIP: ""
      externalTrafficPolicy: Cluster
      extraPorts: []
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      nodePorts:
        redis: ""
        sentinel: ""
      ports:
        redis: 6379
        sentinel: 26379
      type: ClusterIP
    startupProbe:
      enabled: true
      failureThreshold: 22
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    terminationGracePeriodSeconds: 30
  serviceAccount:
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: ""
  sysctl:
    command: []
    enabled: false
    image:
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/bitnami-shell
      tag: 10-debian-10-r408
    mountHostSys: false
    resources:
      limits: {}
      requests: {}
  tls:
    authClients: true
    autoGenerated: false
    certCAFilename: ""
    certFilename: ""
    certKeyFilename: ""
    certificatesSecret: ""
    dhParamsFilename: ""
    enabled: false
    existingSecret: ""
  useExternalDNS:
    additionalAnnotations: {}
    annotationKey: external-dns.alpha.kubernetes.io/
    enabled: false
    suffix: ""
  volumePermissions:
    containerSecurityContext:
      runAsUser: 0
    enabled: false
    image:
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: bitnami/bitnami-shell
      tag: 10-debian-10-r408
    resources:
      limits: {}
      requests: {}
saml:
  disabled: false
  enforced: false
service:
  annotations: {}
  externalPort: 8000
  internalPort: 8000
  name: analytickit
  type: NodePort
serviceAccount:
  annotations: {}
  create: true
siteUrl: dpa.analytickit.com
web:
  affinity: {}
  enabled: true
  env:
  - name: SOCIAL_AUTH_GOOGLE_OAUTH2_KEY
    value: null
  - name: SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET
    value: null
  - name: SOCIAL_AUTH_GOOGLE_OAUTH2_WHITELISTED_DOMAINS
    value: analytickit.com
  hpa:
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  internalMetrics:
    capture: true
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 0
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 2
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 0
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 5
  replicacount: 1
  resources: {}
  secureCookies: true
  securityContext:
    enabled: false
  startupProbe:
    failureThreshold: 6
    initialDelaySeconds: 0
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 5
  tolerations: []
worker:
  affinity: {}
  enabled: true
  env: []
  hpa:
    cputhreshold: 60
    enabled: false
    maxpods: 10
    minpods: 1
  nodeSelector: {}
  podSecurityContext:
    enabled: false
  replicacount: 1
  resources: {}
  securityContext:
    enabled: false
  tolerations: []
zookeeper:
  affinity: {}
  allowAnonymousLogin: true
  auth:
    enabled: false
  autopurge:
    purgeInterval: 1
    snapRetainCount: 3
  clusterDomain: cluster.local
  common:
    exampleValue: common-chart
    global: {}
  commonAnnotations: {}
  commonLabels: {}
  customLivenessProbe: {}
  customReadinessProbe: {}
  dataLogDir: ""
  enabled: true
  extraDeploy: []
  fourlwCommandsWhitelist: srvr, mntr, ruok
  global: {}
  heapSize: 1024
  hostAliases: []
  image:
    debug: false
    pullPolicy: IfNotPresent
    registry: docker.io
    repository: bitnami/zookeeper
    tag: 3.7.0-debian-10-r70
  initContainers: []
  initLimit: 10
  listenOnAllIPs: false
  livenessProbe:
    enabled: true
    failureThreshold: 6
    initialDelaySeconds: 30
    periodSeconds: 10
    probeCommandTimeout: 2
    successThreshold: 1
    timeoutSeconds: 5
  logLevel: ERROR
  maxClientCnxns: 60
  maxSessionTimeout: 40000
  metrics:
    containerPort: 9141
    enabled: false
    prometheusRule:
      enabled: false
      rules: []
    service:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: '{{ .Values.metrics.service.port }}'
        prometheus.io/scrape: "false"
      port: 9141
      type: ClusterIP
    serviceMonitor:
      enabled: false
  minServerId: 1
  nameOverride: analytickit-zookeeper
  networkPolicy:
    enabled: false
  nodeAffinityPreset:
    key: ""
    type: ""
    values: []
  nodeSelector: {}
  persistence:
    accessModes:
    - ReadWriteOnce
    annotations: {}
    dataLogDir:
      selector: {}
      size: 8Gi
    enabled: true
    selector: {}
    size: 8Gi
  podAffinityPreset: ""
  podAntiAffinityPreset: soft
  podDisruptionBudget:
    maxUnavailable: 1
  podLabels: {}
  podManagementPolicy: Parallel
  priorityClassName: ""
  readinessProbe:
    enabled: true
    failureThreshold: 6
    initialDelaySeconds: 5
    periodSeconds: 10
    probeCommandTimeout: 2
    successThreshold: 1
    timeoutSeconds: 5
  replicaCount: 1
  resources:
    requests:
      cpu: 250m
      memory: 256Mi
  securityContext:
    enabled: true
    fsGroup: 1001
    runAsUser: 1001
  service:
    annotations: {}
    disableBaseClientPort: false
    electionPort: 3888
    followerPort: 2888
    headless:
      annotations: {}
    nodePorts:
      client: ""
      clientTls: ""
    port: 2181
    publishNotReadyAddresses: true
    tlsClientPort: 3181
    type: ClusterIP
  serviceAccount:
    automountServiceAccountToken: true
    create: false
  syncLimit: 5
  tickTime: 2000
  tls:
    client:
      autoGenerated: false
      enabled: false
      keystorePassword: ""
      keystorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.keystore.jks
      truststorePassword: ""
      truststorePath: /opt/bitnami/zookeeper/config/certs/client/zookeeper.truststore.jks
    quorum:
      autoGenerated: false
      enabled: false
      keystorePassword: ""
      keystorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.keystore.jks
      truststorePassword: ""
      truststorePath: /opt/bitnami/zookeeper/config/certs/quorum/zookeeper.truststore.jks
    resources:
      limits: {}
      requests: {}
  tolerations: []
  updateStrategy: RollingUpdate
  volumePermissions:
    enabled: false
    image:
      pullPolicy: Always
      registry: docker.io
      repository: bitnami/bitnami-shell
      tag: 10-debian-10-r115
    resources: {}

HOOKS:
MANIFEST:
---
# Source: analytickit/charts/kafka/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: analytickit-analytickit-kafka
  labels:
    app.kubernetes.io/name: analytickit-kafka
    helm.sh/chart: kafka-14.9.3
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
automountServiceAccountToken: true
---
# Source: analytickit/charts/redis/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: analytickit-analytickit-redis
  namespace: "default"
  labels:
    app.kubernetes.io/name: analytickit-redis
    helm.sh/chart: redis-16.8.9
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
---
# Source: analytickit/templates/clickhouse-operator/serviceaccount.yaml
# Template Parameters:
#
# COMMENT=
# NAMESPACE=default
# NAME=clickhouse-operator
#
# Setup ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: clickhouse-operator
  namespace: default
  labels:
    clickhouse.altinity.com/chop: 0.18.4
---
# Source: analytickit/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: analytickit
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
---
# Source: analytickit/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: analytickit-analytickit-postgresql
  labels:
    app: analytickit-postgresql
    chart: postgresql-8.6.1
    release: "analytickit"
    heritage: "Helm"
type: Opaque
data:
  postgresql-password: "cG9zdGdyZXM="
---
# Source: analytickit/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: analytickit
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
type: Opaque
data:
  posthog-secret: "MzNDbUNSeUZBejFGekcyVGtlREE1TlE5MXE5dG9jbFJHb1J0U1hDdw=="
  smtp-password: ""
---
# Source: analytickit/charts/kafka/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: analytickit-analytickit-kafka-scripts
  labels:
    app.kubernetes.io/name: analytickit-kafka
    helm.sh/chart: kafka-14.9.3
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
data:
  setup.sh: |-
    #!/bin/bash

    ID="${MY_POD_NAME#"analytickit-analytickit-kafka-"}"
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        export KAFKA_CFG_BROKER_ID="$(grep "broker.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
    else
        export KAFKA_CFG_BROKER_ID="$((ID + 0))"
    fi

    exec /entrypoint.sh /run.sh
---
# Source: analytickit/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: analytickit-analytickit-redis-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/name: analytickit-redis
    helm.sh/chart: redis-16.8.9
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    slave-read-only yes
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: analytickit/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: analytickit-analytickit-redis-health
  namespace: "default"
  labels:
    app.kubernetes.io/name: analytickit-redis
    helm.sh/chart: redis-16.8.9
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: analytickit/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: analytickit-analytickit-redis-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: analytickit-redis
    helm.sh/chart: redis-16.8.9
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    ARGS+=("--maxmemory 400mb")
    ARGS+=("--maxmemory-policy allkeys-lru")
    exec redis-server "${ARGS[@]}"
---
# Source: analytickit/templates/clickhouse-operator/configmap.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-files
# NAMESPACE=default
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: etc-clickhouse-operator-files
  namespace: default
  labels:
    clickhouse.altinity.com/chop: 0.18.4
    app: clickhouse-operator
data:
  config.yaml: |
    # IMPORTANT
    # This file is auto-generated from deploy/builder/templates-config.
    # It will be overwritten upon next sources build.
    #
    # Template parameters available:
    #   watchNamespaces
    #   chUsername
    #   chPassword
    #   password_sha256_hex
    
    ################################################
    ##
    ## Watch Section
    ##
    ################################################
    watch:
      # List of namespaces where clickhouse-operator watches for events.
      # Concurrently running operators should watch on different namespaces
      #namespaces: ["dev", "test"]
      namespaces: []
    
    clickhouse:
      configuration:
        ################################################
        ##
        ## Configuration Files Section
        ##
        ################################################
        file:
          path:
            # Path to the folder where ClickHouse configuration files common for all instances within a CHI are located.
            common: config.d
            # Path to the folder where ClickHouse configuration files unique for each instance (host) within a CHI are located.
            host: conf.d
            # Path to the folder where ClickHouse configuration files with users settings are located.
            # Files are common for all instances within a CHI.
            user: users.d
        ################################################
        ##
        ## Configuration Users Section
        ##
        ################################################
        user:
          default:
            # Default values for ClickHouse user configuration
            # 1. user/profile - string
            # 2. user/quota - string
            # 3. user/networks/ip - multiple strings
            # 4. user/password - string
            profile: default
            quota: default
            networksIP:
              - "::1"
              - "127.0.0.1"
            password: "default"
        ################################################
        ##
        ## Configuration Network Section
        ##
        ################################################
        network:
          # Default host_regexp to limit network connectivity from outside
          hostRegexpTemplate: "(chi-{chi}-[^.]+\\d+-\\d+|clickhouse\\-{chi})\\.{namespace}\\.svc\\.cluster\\.local$"
      ################################################
      ##
      ## Access to ClickHouse instances
      ##
      ################################################
      access:
        # ClickHouse credentials (username, password and port) to be used by operator to connect to ClickHouse instances
        # for:
        # 1. Metrics requests
        # 2. Schema maintenance
        # 3. DROP DNS CACHE
        # User with such credentials can be specified in additional ClickHouse .xml config files,
        # located in `chUsersConfigsPath` folder
        username: "clickhouse_operator"
        password: "clickhouse_operator_password"
        secret:
          # Location of k8s Secret with username and password to be used by operator to connect to ClickHouse instances
          # Can be used instead of explicitly specified username and password
          namespace: ""
          name: ""
        # Port where to connect to ClickHouse instances to
        port: 8123
    
    ################################################
    ##
    ## Templates Section
    ##
    ################################################
    template:
      chi:
        # Path to the folder where ClickHouseInstallation .yaml manifests are located.
        # Manifests are applied in sorted alpha-numeric order.
        path: templates.d
    
    ################################################
    ##
    ## Reconcile Section
    ##
    ################################################
    reconcile:
      runtime:
        # Max number of concurrent reconciles in progress
        threadsNumber: 10
    
      statefulSet:
        create:
          # What to do in case created StatefulSet is not in Ready after `statefulSetUpdateTimeout` seconds
          # Possible options:
          # 1. abort - do nothing, just break the process and wait for admin
          # 2. delete - delete newly created problematic StatefulSet
          # 3. ignore - ignore error, pretend nothing happened and move on to the next StatefulSet
          onFailure: ignore
    
        update:
          # How many seconds to wait for created/updated StatefulSet to be Ready
          timeout: 300
          # How many seconds to wait between checks for created/updated StatefulSet status
          pollInterval: 5
          # What to do in case updated StatefulSet is not in Ready after `statefulSetUpdateTimeout` seconds
          # Possible options:
          # 1. abort - do nothing, just break the process and wait for admin
          # 2. rollback - delete Pod and rollback StatefulSet to previous Generation.
          # Pod would be recreated by StatefulSet based on rollback-ed configuration
          # 3. ignore - ignore error, pretend nothing happened and move on to the next StatefulSet
          onFailure: rollback
    
      host:
        # Whether reconciler should wait for host:
        # to be excluded from cluster OR
        # to be included into cluster
        # respectfully
        wait:
          exclude: true
          include: false
    
    ################################################
    ##
    ## Annotations management
    ##
    ################################################
    annotation:
      # Applied when:
      #  1. Propagating annotations from the CHI's `metadata.annotations` to child objects' `metadata.annotations`,
      #  2. Propagating annotations from the CHI Template's `metadata.annotations` to CHI's `metadata.annotations`,
      # Include annotations from the following list:
      # Applied only when not empty. Empty list means "include all, no selection"
      include: []
      # Exclude annotations from the following list:
      exclude: []
    
    ################################################
    ##
    ## Labels management
    ##
    ################################################
    label:
      # Applied when:
      #  1. Propagating labels from the CHI's `metadata.labels` to child objects' `metadata.labels`,
      #  2. Propagating labels from the CHI Template's `metadata.labels` to CHI's `metadata.labels`,
      # Include labels from the following list:
      # Applied only when not empty. Empty list means "include all, no selection"
      include: []
      # Exclude labels from the following list:
      exclude: []
      # Whether to append *Scope* labels to StatefulSet and Pod.
      # Full list of available *scope* labels check in labeler.go
      #  LabelShardScopeIndex
      #  LabelReplicaScopeIndex
      #  LabelCHIScopeIndex
      #  LabelCHIScopeCycleSize
      #  LabelCHIScopeCycleIndex
      #  LabelCHIScopeCycleOffset
      #  LabelClusterScopeIndex
      #  LabelClusterScopeCycleSize
      #  LabelClusterScopeCycleIndex
      #  LabelClusterScopeCycleOffset
      appendScope: "no"
    
    ################################################
    ##
    ## StatefulSet management
    ##
    ################################################
    statefulSet:
      revisionHistoryLimit: 0
    
    ################################################
    ##
    ## Pod management
    ##
    ################################################
    pod:
      # Grace period for Pod termination.
      # How many seconds to wait between sending
      # SIGTERM and SIGKILL during Pod termination process.
      # Increase this number is case of slow shutdown.
      terminationGracePeriod: 30
    
    ################################################
    ##
    ## Log parameters
    ##
    ################################################
    logger:
      logtostderr: "true"
      alsologtostderr: "false"
      v: "1"
      stderrthreshold: ""
      vmodule: ""
      log_backtrace_at: ""
---
# Source: analytickit/templates/clickhouse-operator/configmap.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-confd-files
# NAMESPACE=default
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: etc-clickhouse-operator-confd-files
  namespace: default
  labels:
    clickhouse.altinity.com/chop: 0.18.4
    app: clickhouse-operator
data:
---
# Source: analytickit/templates/clickhouse-operator/configmap.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-configd-files
# NAMESPACE=default
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: etc-clickhouse-operator-configd-files
  namespace: default
  labels:
    clickhouse.altinity.com/chop: 0.18.4
    app: clickhouse-operator
data:
  01-clickhouse-01-listen.xml: |
    <yandex>
        <!-- Listen wildcard address to allow accepting connections from other containers and host network. -->
        <listen_host>::</listen_host>
        <listen_host>0.0.0.0</listen_host>
        <listen_try>1</listen_try>
    </yandex>

  01-clickhouse-02-logger.xml: |
    <yandex>
        <logger>
            <!-- Possible levels: https://github.com/pocoproject/poco/blob/develop/Foundation/include/Poco/Logger.h#L105 -->
            <level>debug</level>
            <log>/var/log/clickhouse-server/clickhouse-server.log</log>
            <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
            <size>1000M</size>
            <count>10</count>
            <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->
            <console>1</console>
        </logger>
    </yandex>

  01-clickhouse-03-query_log.xml: |
    <yandex>
        <query_log replace="1">
            <database>system</database>
            <table>query_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_log>
        <query_thread_log remove="1"/>
    </yandex>

  01-clickhouse-04-part_log.xml: |
    <yandex>
        <part_log replace="1">
            <database>system</database>
            <table>part_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </part_log>
    </yandex>
---
# Source: analytickit/templates/clickhouse-operator/configmap.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-templatesd-files
# NAMESPACE=default
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: etc-clickhouse-operator-templatesd-files
  namespace: default
  labels:
    clickhouse.altinity.com/chop: 0.18.4
    app: clickhouse-operator
data:
  001-templates.json.example: |
    {
      "apiVersion": "clickhouse.altinity.com/v1",
      "kind": "ClickHouseInstallationTemplate",
      "metadata": {
        "name": "01-default-volumeclaimtemplate"
      },
      "spec": {
        "templates": {
          "volumeClaimTemplates": [
            {
              "name": "chi-default-volume-claim-template",
              "spec": {
                "accessModes": [
                  "ReadWriteOnce"
                ],
                "resources": {
                  "requests": {
                    "storage": "2Gi"
                  }
                }
              }
            }
          ],
          "podTemplates": [
            {
              "name": "chi-default-oneperhost-pod-template",
              "distribution": "OnePerHost",
              "spec": {
                "containers" : [
                  {
                    "name": "clickhouse",
                    "image": "clickhouse/clickhouse-server:22.3",
                    "ports": [
                      {
                        "name": "http",
                        "containerPort": 8123
                      },
                      {
                        "name": "client",
                        "containerPort": 9000
                      },
                      {
                        "name": "interserver",
                        "containerPort": 9009
                      }
                    ]
                  }
                ]
              }
            }
          ]
        }
      }
    }

  default-pod-template.yaml.example: |
    apiVersion: "clickhouse.altinity.com/v1"
    kind: "ClickHouseInstallationTemplate"
    metadata:
      name: "default-oneperhost-pod-template"
    spec:
      templates:
        podTemplates:
          - name: default-oneperhost-pod-template
            distribution: "OnePerHost"
  default-storage-template.yaml.example: |
    apiVersion: "clickhouse.altinity.com/v1"
    kind: "ClickHouseInstallationTemplate"
    metadata:
      name: "default-storage-template-2Gi"
    spec:
      templates:
        volumeClaimTemplates:
          - name: default-storage-template-2Gi
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 2Gi

  readme: |
    Templates in this folder are packaged with an operator and available via 'useTemplate'
---
# Source: analytickit/templates/clickhouse-operator/configmap.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-usersd-files
# NAMESPACE=default
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: etc-clickhouse-operator-usersd-files
  namespace: default
  labels:
    clickhouse.altinity.com/chop: 0.18.4
    app: clickhouse-operator
data:
  01-clickhouse-user.xml: |
    <yandex>
        <users>
            <clickhouse_operator>
                <networks>
                    <ip>127.0.0.1</ip>
                    <ip>0.0.0.0/0</ip>
                    <ip>::/0</ip>
                </networks>
                <password_sha256_hex>716b36073a90c6fe1d445ac1af85f4777c5b7a155cea359961826a030513e448</password_sha256_hex>
                <profile>clickhouse_operator</profile>
                <quota>default</quota>
            </clickhouse_operator>
        </users>
        <profiles>
            <clickhouse_operator>
                <log_queries>0</log_queries>
                <skip_unavailable_shards>1</skip_unavailable_shards>
                <http_connection_timeout>10</http_connection_timeout>
            </clickhouse_operator>
        </profiles>
    </yandex>

  02-clickhouse-default-profile.xml: |
    <yandex>
      <profiles>
        <default>
          <log_queries>1</log_queries>
          <connect_timeout_with_failover_ms>1000</connect_timeout_with_failover_ms>
          <distributed_aggregation_memory_efficient>1</distributed_aggregation_memory_efficient>
          <parallel_view_processing>1</parallel_view_processing>
        </default>
      </profiles>
    </yandex>
  03-database-ordinary.xml: |
    <!--  Remove it for ClickHouse versions before 20.4 -->
    <yandex>
        <profiles>
            <default>
                <default_database_engine>Ordinary</default_database_engine>
            </default>
        </profiles>
    </yandex>
---
# Source: analytickit/templates/clickhouse-operator/clusterrole.yaml
# Template Parameters:
#
# NAMESPACE=default
# COMMENT=#
# ROLE_KIND=ClusterRole
# ROLE_NAME=clickhouse-operator-default
# ROLE_BINDING_KIND=ClusterRoleBinding
# ROLE_BINDING_NAME=clickhouse-operator-default
#
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: clickhouse-operator-default
  namespace: default
  labels:
    clickhouse.altinity.com/chop: 0.18.4
rules:
- apiGroups:
    - ""
  resources:
    - configmaps
    - services
  verbs:
    - get
    - list
    - patch
    - update
    - watch
    - create
    - delete
- apiGroups:
    - ""
  resources:
    - endpoints
  verbs:
    - get
    - list
    - watch
- apiGroups:
    - ""
  resources:
    - events
  verbs:
    - create
- apiGroups:
    - ""
  resources:
    - persistentvolumeclaims
  verbs:
    - get
    - list
    - patch
    - update
    - watch
    - delete
- apiGroups:
    - ""
  resources:
    - persistentvolumes
    - pods
  verbs:
    - get
    - list
    - patch
    - update
    - watch
- apiGroups:
    - apps
  resources:
    - statefulsets
  verbs:
    - get
    - list
    - patch
    - update
    - watch
    - create
    - delete
- apiGroups:
    - apps
  resources:
    - replicasets
  verbs:
    - get
    - patch
    - update
    - delete
- apiGroups:
    - apps
  resourceNames:
    - clickhouse-operator
  resources:
    - deployments
  verbs:
    - get
    - patch
    - update
    - delete
- apiGroups:
    - policy
  resources:
    - poddisruptionbudgets
  verbs:
    - get
    - list
    - patch
    - update
    - watch
    - create
    - delete
- apiGroups:
    - clickhouse.altinity.com
  resources:
    - clickhouseinstallations
  verbs:
    - get
    - patch
    - update
    - delete
- apiGroups:
    - clickhouse.altinity.com
  resources:
    - clickhouseinstallations
    - clickhouseinstallationtemplates
    - clickhouseoperatorconfigurations
  verbs:
    - get
    - list
    - watch
- apiGroups:
    - clickhouse.altinity.com
  resources:
    - clickhouseinstallations/finalizers
    - clickhouseinstallationtemplates/finalizers
    - clickhouseoperatorconfigurations/finalizers
  verbs:
    - update
- apiGroups:
    - clickhouse.altinity.com
  resources:
    - clickhouseinstallations/status
    - clickhouseinstallationtemplates/status
    - clickhouseoperatorconfigurations/status
  verbs:
    - get
    - update
    - patch
    - create
    - delete
- apiGroups:
    - ""
  resources:
    - secrets
  verbs:
    - get
    - list
- apiGroups:
    - apiextensions.k8s.io
  resources:
    - customresourcedefinitions
  verbs:
    - get
    - list
---
# Source: analytickit/templates/clickhouse-operator/clusterrolebinding.yaml
# Setup ClusterRoleBinding between ClusterRole and ServiceAccount.
# ClusterRoleBinding is namespace-less and must have unique name
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: clickhouse-operator-default
  namespace: default
  labels:
    clickhouse.altinity.com/chop: 0.18.4
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: clickhouse-operator-default
subjects:
- kind: ServiceAccount
  name: clickhouse-operator
  namespace: default
---
# Source: analytickit/charts/kafka/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: analytickit-analytickit-kafka-headless
  labels:
    app.kubernetes.io/name: analytickit-kafka
    helm.sh/chart: kafka-14.9.3
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
    - name: tcp-internal
      port: 9093
      protocol: TCP
      targetPort: kafka-internal
  selector:
    app.kubernetes.io/name: analytickit-kafka
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/component: kafka
---
# Source: analytickit/charts/kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: analytickit-analytickit-kafka
  labels:
    app.kubernetes.io/name: analytickit-kafka
    helm.sh/chart: kafka-14.9.3
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
      nodePort: null
  selector:
    app.kubernetes.io/name: analytickit-kafka
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/component: kafka
---
# Source: analytickit/charts/postgresql/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: analytickit-analytickit-postgresql-headless
  labels:
    app: analytickit-postgresql
    chart: postgresql-8.6.1
    release: "analytickit"
    heritage: "Helm"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: analytickit-postgresql
    release: "analytickit"
---
# Source: analytickit/charts/postgresql/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: analytickit-analytickit-postgresql
  labels:
    app: analytickit-postgresql
    chart: postgresql-8.6.1
    release: "analytickit"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: analytickit-postgresql
    release: "analytickit"
    role: master
---
# Source: analytickit/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: analytickit-analytickit-redis-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: analytickit-redis
    helm.sh/chart: redis-16.8.9
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
  annotations:
    
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/name: analytickit-redis
    app.kubernetes.io/instance: analytickit
---
# Source: analytickit/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: analytickit-analytickit-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: analytickit-redis
    helm.sh/chart: redis-16.8.9
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  
  internalTrafficPolicy: Cluster
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: analytickit-redis
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/component: master
---
# Source: analytickit/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: analytickit-analytickit-zookeeper-headless
  namespace: default
  labels:
    app.kubernetes.io/name: analytickit-zookeeper
    helm.sh/chart: zookeeper-7.0.5
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    
    - name: tcp-client
      port: 2181
      targetPort: client
    
    
    - name: follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: analytickit-zookeeper
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/component: zookeeper
---
# Source: analytickit/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: analytickit-analytickit-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/name: analytickit-zookeeper
    helm.sh/chart: zookeeper-7.0.5
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  ports:
    
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    
    
    - name: follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: analytickit-zookeeper
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/component: zookeeper
---
# Source: analytickit/templates/clickhouse-operator/service.yaml
# Template Parameters:
#
# NAMESPACE=default
# COMMENT=
#
# Setup ClusterIP Service to provide monitoring metrics for Prometheus
# Service would be created in kubectl-specified namespace
# In order to get access outside of k8s it should be exposed as:
# kubectl --namespace prometheus port-forward service/prometheus 9090
# and point browser to localhost:9090
kind: Service
apiVersion: v1
metadata:
  name: clickhouse-operator-metrics
  namespace: default
  labels:
    clickhouse.altinity.com/chop: 0.18.4
    app: clickhouse-operator
spec:
  ports:
    - port: 8888
      name: clickhouse-operator-metrics
  selector:
    app: clickhouse-operator
---
# Source: analytickit/templates/events-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: analytickit-events
  annotations:
    
    "meta.helm.sh/release-name": "analytickit"
    "meta.helm.sh/release-namespace": "default"
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
spec:
  type: NodePort
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: analytickit
  selector:
    app: analytickit
    role: events
---
# Source: analytickit/templates/pgbouncer-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: analytickit-pgbouncer
  annotations:
    
    "meta.helm.sh/release-name": "analytickit"
    "meta.helm.sh/release-namespace": "default"
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
spec:
  type: ClusterIP
  ports:
  - name: analytickit-pgbouncer
    port: 6543
    targetPort: 6543
  selector:
    app: analytickit
    role: pgbouncer
---
# Source: analytickit/templates/web-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: analytickit-web
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
  annotations:
    
    "meta.helm.sh/release-name": "analytickit"
    "meta.helm.sh/release-namespace": "default"
spec:
  type: NodePort
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: analytickit
  selector:
    app: analytickit
    role: web
---
# Source: analytickit/templates/clickhouse-operator/deployment.yaml
# Template Parameters:
#
# NAMESPACE=default
# COMMENT=
# OPERATOR_IMAGE=altinity/clickhouse-operator:0.19.0
# METRICS_EXPORTER_IMAGE=altinity/metrics-exporter:latest
#
# Setup Deployment for clickhouse-operator
# Deployment would be created in kubectl-specified namespace
kind: Deployment
apiVersion: apps/v1
metadata:
  name: clickhouse-operator
  namespace: default
  labels:
    clickhouse.altinity.com/chop: 0.18.4
    app: clickhouse-operator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: clickhouse-operator
  template:
    metadata:
      labels:
        app: clickhouse-operator
      annotations:
        prometheus.io/port: '8888'
        prometheus.io/scrape: 'true'
    spec:
      serviceAccountName: clickhouse-operator
      volumes:
        - name: etc-clickhouse-operator-folder
          configMap:
            name: etc-clickhouse-operator-files
        - name: etc-clickhouse-operator-confd-folder
          configMap:
            name: etc-clickhouse-operator-confd-files
        - name: etc-clickhouse-operator-configd-folder
          configMap:
            name: etc-clickhouse-operator-configd-files
        - name: etc-clickhouse-operator-templatesd-folder
          configMap:
            name: etc-clickhouse-operator-templatesd-files
        - name: etc-clickhouse-operator-usersd-folder
          configMap:
            name: etc-clickhouse-operator-usersd-files
      containers:
        - name: clickhouse-operator
          image: altinity/clickhouse-operator:0.19.0
          imagePullPolicy: Always
          volumeMounts:
            - name: etc-clickhouse-operator-folder
              mountPath: /etc/clickhouse-operator
            - name: etc-clickhouse-operator-confd-folder
              mountPath: /etc/clickhouse-operator/conf.d
            - name: etc-clickhouse-operator-configd-folder
              mountPath: /etc/clickhouse-operator/config.d
            - name: etc-clickhouse-operator-templatesd-folder
              mountPath: /etc/clickhouse-operator/templates.d
            - name: etc-clickhouse-operator-usersd-folder
              mountPath: /etc/clickhouse-operator/users.d
          env:
            # Pod-specific
            # spec.nodeName: ip-172-20-52-62.ec2.internal
            - name: OPERATOR_POD_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # metadata.name: clickhouse-operator-6f87589dbb-ftcsf
            - name: OPERATOR_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            # metadata.namespace: kube-system
            - name: OPERATOR_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            # status.podIP: 100.96.3.2
            - name: OPERATOR_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            # spec.serviceAccount: clickhouse-operator
            # spec.serviceAccountName: clickhouse-operator
            - name: OPERATOR_POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName

            # Container-specific
            - name: OPERATOR_CONTAINER_CPU_REQUEST
              valueFrom:
                resourceFieldRef:
                  containerName: clickhouse-operator
                  resource: requests.cpu
            - name: OPERATOR_CONTAINER_CPU_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: clickhouse-operator
                  resource: limits.cpu
            - name: OPERATOR_CONTAINER_MEM_REQUEST
              valueFrom:
                resourceFieldRef:
                  containerName: clickhouse-operator
                  resource: requests.memory
            - name: OPERATOR_CONTAINER_MEM_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: clickhouse-operator
                  resource: limits.memory

        - name: metrics-exporter
          image: altinity/metrics-exporter:latest
          imagePullPolicy: Always
          volumeMounts:
            - name: etc-clickhouse-operator-folder
              mountPath: /etc/clickhouse-operator
            - name: etc-clickhouse-operator-confd-folder
              mountPath: /etc/clickhouse-operator/conf.d
            - name: etc-clickhouse-operator-configd-folder
              mountPath: /etc/clickhouse-operator/config.d
            - name: etc-clickhouse-operator-templatesd-folder
              mountPath: /etc/clickhouse-operator/templates.d
            - name: etc-clickhouse-operator-usersd-folder
              mountPath: /etc/clickhouse-operator/users.d
          ports:
            - containerPort: 8888
              name: metrics
---
# Source: analytickit/templates/events-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: analytickit-events
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
  annotations:
    
    "meta.helm.sh/release-name": "analytickit"
    "meta.helm.sh/release-namespace": "default"
spec:
  selector:
    matchLabels:
        app: analytickit
        release: "analytickit"
        role: events
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/secrets.yaml: f8cc990d79a5a29d1c4c6e9ebda9ba77a926206ab9dfaab80e4d7305bb068f2f
      labels:
        app: analytickit
        release: "analytickit"
        role: events
    spec:
      terminationGracePeriodSeconds: 45
      serviceAccountName: analytickit
      containers:
      - name: analytickit-events
        image: "analytickit/analytickit:release-1.41.3"
        command:
          - ./bin/docker-server
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
        # Expose the port on which Prometheus /metrics endpoint resides
        - containerPort: 8001
        env:
        # Kafka env variables        
        # Used by PostHog/plugin-server. There is no specific reason for the difference. Expected format: comma-separated list of "host:port"
        - name: KAFKA_HOSTS
          value: analytickit-analytickit-kafka:9092
        
        # Used by PostHog/web. There is no specific reason for the difference. Expected format: comma-separated list of "kafka://host:port"
        - name: KAFKA_URL
          value: kafka://analytickit-analytickit-kafka:9092
        

        # Object Storage env variables        
        
        

        # Redis env variables        
        
        - name: ANALYTICKIT_REDIS_HOST
          value: analytickit-analytickit-redis-master
        
        - name: ANALYTICKIT_REDIS_PORT
          value: "6379"

        # statsd env variables        

        - name: DISABLE_SECURE_SSL_REDIRECT
          value: '1'
        - name: IS_BEHIND_PROXY
          value: '1'
        # PostHog app settings        
        - name: SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: analytickit
              key: analytickit-secret
        - name: SITE_URL
          value: dpa.analytickit.com
        - name: DEPLOYMENT
          value: helm_aws_ha
        - name: CAPTURE_INTERNAL_METRICS
          value: "true"
        - name: HELM_INSTALL_INFO
          value: "{\"chart_version\":\"0.0.1\",\"cloud\":\"aws\",\"deployment_type\":\"helm\",\"hostname\":null,\"ingress_type\":\"\",\"kube_version\":\"v1.23.13-eks-fb459a0\",\"operation\":\"install\",\"release_name\":\"analytickit\",\"release_revision\":1}"
        - name: LOGGING_FORMATTER_NAME
          value: json        
        - name: SENTRY_DSN
          value: 
        - name: PRIMARY_DB
          value: clickhouse
        
        - name: ANALYTICKIT_POSTGRES_HOST
          value: analytickit-pgbouncer
        - name: ANALYTICKIT_POSTGRES_PORT
          value: "6543"
        - name: ANALYTICKIT_DB_USER
          value: "postgres"
        - name: ANALYTICKIT_DB_NAME
          value: "analytickit"
        - name: ANALYTICKIT_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: analytickit-analytickit-postgresql
              key: "postgresql-password"
        - name: USING_PGBOUNCER
          value: 'true'
        - name: CLICKHOUSE_HOST
          value: clickhouse-analytickit
        - name: CLICKHOUSE_CLUSTER
          value: "analytickit"
        - name: CLICKHOUSE_DATABASE
          value: "analytickit"
        - name: CLICKHOUSE_USER
          value: "admin"
        - name: CLICKHOUSE_PASSWORD
          value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
        - name: CLICKHOUSE_SECURE
          value: "false"
        - name: CLICKHOUSE_VERIFY
          value: "false"
        
        - name: EMAIL_HOST
          value: ""
        - name: EMAIL_PORT
          value: ""
        - name: EMAIL_HOST_USER
          value: ""
        - name: EMAIL_HOST_PASSWORD
          valueFrom:
            secretKeyRef:
              name: analytickit
              key: "smtp-password"
        - name: EMAIL_USE_TLS
          value: "true"
        - name: EMAIL_USE_SSL
          value: "false"
        - name: DEFAULT_FROM_EMAIL
          value: 
        - name: SOCIAL_AUTH_GOOGLE_OAUTH2_KEY
          value: null
        - name: SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET
          value: null
        - name: SOCIAL_AUTH_GOOGLE_OAUTH2_WHITELISTED_DOMAINS
          value: analytickit.com
        lifecycle:
            preStop:
                exec:
                    command: [
                        "sh", "-c",
                        "(echo '{\"event\": \"preStop_started\"}'; sleep 10; echo '{\"event\": \"preStop_ended\"}') > /proc/1/fd/1"
                    ]

        livenessProbe:
          httpGet:
            path: /_livez
            port: 8000
            scheme: HTTP
          failureThreshold: 3
          initialDelaySeconds: 0
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 2
        readinessProbe:
          httpGet:
            # For readiness, we want to use the checks specific to the events
            # role, which may be a subset of all the apps dependencies
            path: /_readyz?role=events
            port: 8000
            scheme: HTTP
          failureThreshold: 3
          initialDelaySeconds: 0
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        startupProbe:
          httpGet:
            # For startup, we want to make sure that everything is in place,
            # including postgres. This does however mean we would not be able to
            # deploy new releases when we have a postgres outage
            path: /_readyz
            port: 8000
            scheme: HTTP
          failureThreshold: 6
          initialDelaySeconds: 0
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
        
            {}
        
      initContainers:        
        - name: wait-for-service-dependencies
          image: busybox:1.34
          imagePullPolicy: IfNotPresent
          env:
            - name: CLICKHOUSE_HOST
              value: clickhouse-analytickit
            - name: CLICKHOUSE_CLUSTER
              value: "analytickit"
            - name: CLICKHOUSE_DATABASE
              value: "analytickit"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
            - name: CLICKHOUSE_SECURE
              value: "false"
            - name: CLICKHOUSE_VERIFY
              value: "false"
          command:
            - /bin/sh
            - -c
            - >
                
                until (
                    NODES_COUNT=$(wget -qO- \
                        "http://$CLICKHOUSE_USER:$CLICKHOUSE_PASSWORD@clickhouse-analytickit.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local:8123" \
                        --post-data "SELECT count() FROM clusterAllReplicas('analytickit', system, one)"
                    )
                    test ! -z $NODES_COUNT && test $NODES_COUNT -eq 1
                );
                do
                    echo "waiting for all ClickHouse nodes to be available"; sleep 1;
                done
                
        
                until (nc -vz "analytickit-pgbouncer.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" 6543);
                do
                    echo "waiting for PgBouncer"; sleep 1;
                done
        
                
                until (nc -vz "analytickit-analytickit-postgresql.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" 5432);
                do
                    echo "waiting for PostgreSQL"; sleep 1;
                done
                
        
                
                until (nc -vz "analytickit-analytickit-redis-master.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" "6379");
                do
                    echo "waiting for Redis"; sleep 1;
                done
                
        
                
        
                KAFKA_BROKERS="analytickit-analytickit-kafka:9092"
        
                KAFKA_HOST=$(echo $KAFKA_BROKERS | cut -f1 -d:)
                KAFKA_PORT=$(echo $KAFKA_BROKERS | cut -f2 -d:)
        
                until (nc -vz "$KAFKA_HOST.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" $KAFKA_PORT);
                do
                    echo "waiting for Kafka"; sleep 1;
                done
                        
        - name: wait-for-migrations
          image: "analytickit/analytickit:release-1.41.3"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - >
                until (python manage.py migrate --check);
                do
                    echo "Waiting for PostgreSQL database migrations to be completed"; sleep 1;
                done
        
                until (python manage.py migrate_clickhouse --check);
                do
                    echo "Waiting for ClickHouse database migrations to be completed";
                    sleep 1;
                done
          env:
        
          # PostgreSQL configuration
          
          - name: ANALYTICKIT_POSTGRES_HOST
            value: analytickit-pgbouncer
          - name: ANALYTICKIT_POSTGRES_PORT
            value: "6543"
          - name: ANALYTICKIT_DB_USER
            value: "postgres"
          - name: ANALYTICKIT_DB_NAME
            value: "analytickit"
          - name: ANALYTICKIT_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: analytickit-analytickit-postgresql
                key: "postgresql-password"
          - name: USING_PGBOUNCER
            value: 'true'
        
          # Redis env variables  
          
          - name: ANALYTICKIT_REDIS_HOST
            value: analytickit-analytickit-redis-master
          
          - name: ANALYTICKIT_REDIS_PORT
            value: "6379"
        
          # ClickHouse env variables
          - name: CLICKHOUSE_HOST
            value: clickhouse-analytickit
          - name: CLICKHOUSE_CLUSTER
            value: "analytickit"
          - name: CLICKHOUSE_DATABASE
            value: "analytickit"
          - name: CLICKHOUSE_USER
            value: "admin"
          - name: CLICKHOUSE_PASSWORD
            value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
          - name: CLICKHOUSE_SECURE
            value: "false"
          - name: CLICKHOUSE_VERIFY
            value: "false"
        
          # PostHog app settings
          
          - name: SECRET_KEY
            valueFrom:
              secretKeyRef:
                name: analytickit
                key: analytickit-secret
          - name: SITE_URL
            value: dpa.analytickit.com
          - name: DEPLOYMENT
            value: helm_aws_ha
          - name: CAPTURE_INTERNAL_METRICS
            value: "true"
          - name: HELM_INSTALL_INFO
            value: "{\"chart_version\":\"0.0.1\",\"cloud\":\"aws\",\"deployment_type\":\"helm\",\"hostname\":null,\"ingress_type\":\"\",\"kube_version\":\"v1.23.13-eks-fb459a0\",\"operation\":\"install\",\"release_name\":\"analytickit\",\"release_revision\":1}"
          - name: LOGGING_FORMATTER_NAME
            value: json
          
          - name: SENTRY_DSN
            value: 
        
          # Global ENV variables
---
# Source: analytickit/templates/pgbouncer-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: analytickit-pgbouncer
  annotations:
    
    "meta.helm.sh/release-name": "analytickit"
    "meta.helm.sh/release-namespace": "default"
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
spec:
  selector:
    matchLabels:
      app: analytickit
      release: "analytickit"
      role: pgbouncer
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/secrets.yaml: 9e708420e1a042d9027c168ddf9bf6c2c33229cb760fa119af750c4a6346bca3

        

      labels:
        app: analytickit
        release: "analytickit"
        role: pgbouncer
    spec:
      # Time to wait before hard killing the container. Note: if the container
      # shuts down and exits before the terminationGracePeriod is done, we
      # moves to the next step immediately.
      terminationGracePeriodSeconds: 65

      serviceAccountName: analytickit
      containers:

      - name: analytickit-pgbouncer
        image: "bitnami/pgbouncer:1.17.0"
        imagePullPolicy: IfNotPresent
        ports:
        - name: pgbouncer
          containerPort: 6543
        env:
        - name: POSTGRESQL_USERNAME
          value: "postgres"
        - name: POSTGRESQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: analytickit-analytickit-postgresql
              key: "postgresql-password"
        - name: POSTGRESQL_DATABASE
          value: "analytickit"
        - name: POSTGRESQL_HOST
          value: analytickit-analytickit-postgresql
        - name: POSTGRESQL_PORT
          value: "5432"

        - name: PGBOUNCER_DATABASE
          value: "analytickit"
        
        - name: PGBOUNCER_PORT
          value: "6543"
        - name: PGBOUNCER_MAX_CLIENT_CONN
          value: "1000"
        - name: PGBOUNCER_POOL_MODE
          value: transaction
        - name: PGBOUNCER_IGNORE_STARTUP_PARAMETERS
          value: extra_float_digits

        readinessProbe:
          tcpSocket:
            port: 6543
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 2

        livenessProbe:
          tcpSocket:
            port: 6543
          failureThreshold: 3
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2

        lifecycle:
          preStop:
            exec:
              command: [
                "sh", "-c",
                #
                # Introduce a delay to the shutdown sequence to wait for the
                # pod eviction event to propagate into the cluster.
                #
                # See: https://blog.gruntwork.io/delaying-shutdown-to-wait-for-pod-deletion-propagation-445f779a8304
                #
                #
                # Then, gracefully shutdown pgbouncer by sending a SIGINT
                # to the process (see https://www.pgbouncer.org/usage.html)
                # and sleep again for max query timeout + 1s.
                #
                # Note: once the connections are all drained, the process will
                # exit before the 'sleep 31' completes and the pod will be
                # removed. Unfortunately we will also get an ugly 'FailedPreStopHook'
                # warning in the k8s event logs but I'm not sure how we can avoid it.
                #
                "sleep 30 && kill -INT 1 && sleep 31"
              ]
---
# Source: analytickit/templates/plugins-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: analytickit-plugins
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
  annotations:
    
    "meta.helm.sh/release-name": "analytickit"
    "meta.helm.sh/release-namespace": "default"
spec:
  selector:
    matchLabels:
        app: analytickit
        release: "analytickit"
        role: plugins
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/secrets.yaml: 3b45a7b7bbaa9e464473ae997f4fee0a3665c7599d17cd9cb33187579ba4398b
      labels:
        app: analytickit
        release: "analytickit"
        role: plugins
    spec:
      serviceAccountName: analytickit
      containers:
      - name: analytickit-plugins
        image: "analytickit/analytickit:release-1.41.3"
        imagePullPolicy: IfNotPresent
        command:
          - ./bin/plugin-server
          - --no-restart-loop
        ports:
        # Expose the port on which the healtchheck endpoint listens
        - containerPort: 6738
        env:
        

        - name: SENTRY_DSN
          value: 

        # Kafka env variables
        
        # Used by PostHog/plugin-server. There is no specific reason for the difference. Expected format: comma-separated list of "host:port"
        - name: KAFKA_HOSTS
          value: analytickit-analytickit-kafka:9092
        
        # Used by PostHog/web. There is no specific reason for the difference. Expected format: comma-separated list of "kafka://host:port"
        - name: KAFKA_URL
          value: kafka://analytickit-analytickit-kafka:9092
        

        # Object Storage env variables
        
        
        

        # Redis env variables
        
        
        - name: ANALYTICKIT_REDIS_HOST
          value: analytickit-analytickit-redis-master
        
        - name: ANALYTICKIT_REDIS_PORT
          value: "6379"

        # statsd env variables
                
        - name: SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: analytickit
              key: analytickit-secret
        - name: SITE_URL
          value: dpa.analytickit.com
        - name: DEPLOYMENT
          value: helm_aws_ha
        - name: CAPTURE_INTERNAL_METRICS
          value: "true"
        - name: HELM_INSTALL_INFO
          value: "{\"chart_version\":\"0.0.1\",\"cloud\":\"aws\",\"deployment_type\":\"helm\",\"hostname\":null,\"ingress_type\":\"\",\"kube_version\":\"v1.23.13-eks-fb459a0\",\"operation\":\"install\",\"release_name\":\"analytickit\",\"release_revision\":1}"
        - name: LOGGING_FORMATTER_NAME
          value: json
        
        - name: ANALYTICKIT_POSTGRES_HOST
          value: analytickit-pgbouncer
        - name: ANALYTICKIT_POSTGRES_PORT
          value: "6543"
        - name: ANALYTICKIT_DB_USER
          value: "postgres"
        - name: ANALYTICKIT_DB_NAME
          value: "analytickit"
        - name: ANALYTICKIT_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: analytickit-analytickit-postgresql
              key: "postgresql-password"
        - name: USING_PGBOUNCER
          value: 'true'
        - name: CLICKHOUSE_HOST
          value: clickhouse-analytickit
        - name: CLICKHOUSE_CLUSTER
          value: "analytickit"
        - name: CLICKHOUSE_DATABASE
          value: "analytickit"
        - name: CLICKHOUSE_USER
          value: "admin"
        - name: CLICKHOUSE_PASSWORD
          value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
        - name: CLICKHOUSE_SECURE
          value: "false"
        - name: CLICKHOUSE_VERIFY
          value: "false"
        livenessProbe:
          exec:
            command:
              # Just check that we can at least exec to the container
              - "true"
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /_health
            port: 6738
            scheme: HTTP
          initialDelaySeconds: 50
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        resources:
            {}
      initContainers:        
        - name: wait-for-service-dependencies
          image: busybox:1.34
          imagePullPolicy: IfNotPresent
          env:
            - name: CLICKHOUSE_HOST
              value: clickhouse-analytickit
            - name: CLICKHOUSE_CLUSTER
              value: "analytickit"
            - name: CLICKHOUSE_DATABASE
              value: "analytickit"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
            - name: CLICKHOUSE_SECURE
              value: "false"
            - name: CLICKHOUSE_VERIFY
              value: "false"
          command:
            - /bin/sh
            - -c
            - >
                
                until (
                    NODES_COUNT=$(wget -qO- \
                        "http://$CLICKHOUSE_USER:$CLICKHOUSE_PASSWORD@clickhouse-analytickit.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local:8123" \
                        --post-data "SELECT count() FROM clusterAllReplicas('analytickit', system, one)"
                    )
                    test ! -z $NODES_COUNT && test $NODES_COUNT -eq 1
                );
                do
                    echo "waiting for all ClickHouse nodes to be available"; sleep 1;
                done
                
        
                until (nc -vz "analytickit-pgbouncer.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" 6543);
                do
                    echo "waiting for PgBouncer"; sleep 1;
                done
        
                
                until (nc -vz "analytickit-analytickit-postgresql.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" 5432);
                do
                    echo "waiting for PostgreSQL"; sleep 1;
                done
                
        
                
                until (nc -vz "analytickit-analytickit-redis-master.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" "6379");
                do
                    echo "waiting for Redis"; sleep 1;
                done
                
        
                
        
                KAFKA_BROKERS="analytickit-analytickit-kafka:9092"
        
                KAFKA_HOST=$(echo $KAFKA_BROKERS | cut -f1 -d:)
                KAFKA_PORT=$(echo $KAFKA_BROKERS | cut -f2 -d:)
        
                until (nc -vz "$KAFKA_HOST.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" $KAFKA_PORT);
                do
                    echo "waiting for Kafka"; sleep 1;
                done
                        
        - name: wait-for-migrations
          image: "analytickit/analytickit:release-1.41.3"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - >
                until (python manage.py migrate --check);
                do
                    echo "Waiting for PostgreSQL database migrations to be completed"; sleep 1;
                done
        
                until (python manage.py migrate_clickhouse --check);
                do
                    echo "Waiting for ClickHouse database migrations to be completed";
                    sleep 1;
                done
          env:
        
          # PostgreSQL configuration
          
          - name: ANALYTICKIT_POSTGRES_HOST
            value: analytickit-pgbouncer
          - name: ANALYTICKIT_POSTGRES_PORT
            value: "6543"
          - name: ANALYTICKIT_DB_USER
            value: "postgres"
          - name: ANALYTICKIT_DB_NAME
            value: "analytickit"
          - name: ANALYTICKIT_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: analytickit-analytickit-postgresql
                key: "postgresql-password"
          - name: USING_PGBOUNCER
            value: 'true'
        
          # Redis env variables  
          
          - name: ANALYTICKIT_REDIS_HOST
            value: analytickit-analytickit-redis-master
          
          - name: ANALYTICKIT_REDIS_PORT
            value: "6379"
        
          # ClickHouse env variables
          - name: CLICKHOUSE_HOST
            value: clickhouse-analytickit
          - name: CLICKHOUSE_CLUSTER
            value: "analytickit"
          - name: CLICKHOUSE_DATABASE
            value: "analytickit"
          - name: CLICKHOUSE_USER
            value: "admin"
          - name: CLICKHOUSE_PASSWORD
            value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
          - name: CLICKHOUSE_SECURE
            value: "false"
          - name: CLICKHOUSE_VERIFY
            value: "false"
        
          # PostHog app settings
          
          - name: SECRET_KEY
            valueFrom:
              secretKeyRef:
                name: analytickit
                key: analytickit-secret
          - name: SITE_URL
            value: dpa.analytickit.com
          - name: DEPLOYMENT
            value: helm_aws_ha
          - name: CAPTURE_INTERNAL_METRICS
            value: "true"
          - name: HELM_INSTALL_INFO
            value: "{\"chart_version\":\"0.0.1\",\"cloud\":\"aws\",\"deployment_type\":\"helm\",\"hostname\":null,\"ingress_type\":\"\",\"kube_version\":\"v1.23.13-eks-fb459a0\",\"operation\":\"install\",\"release_name\":\"analytickit\",\"release_revision\":1}"
          - name: LOGGING_FORMATTER_NAME
            value: json
          
          - name: SENTRY_DSN
            value: 
        
          # Global ENV variables
---
# Source: analytickit/templates/web-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: analytickit-web
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
  annotations:
    
    "meta.helm.sh/release-name": "analytickit"
    "meta.helm.sh/release-namespace": "default"
spec:
  selector:
    matchLabels:
        app: analytickit
        release: "analytickit"
        role: web
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/secrets.yaml: d4da4996dae01a2e449486de5b37a9f8c39202219ae1ae4c86f9f8997f4e6156
      labels:
        app: analytickit
        release: "analytickit"
        role: web
    spec:
      terminationGracePeriodSeconds: 45
      serviceAccountName: analytickit
      containers:
      - name: analytickit-web
        image: "analytickit/analytickit:release-1.41.3"
        command:
          - ./bin/docker-server
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
        # Expose the port on which Prometheus /metrics endpoint resides
        - containerPort: 8001
        env:
        # Kafka env variables        
        # Used by PostHog/plugin-server. There is no specific reason for the difference. Expected format: comma-separated list of "host:port"
        - name: KAFKA_HOSTS
          value: analytickit-analytickit-kafka:9092
        
        # Used by PostHog/web. There is no specific reason for the difference. Expected format: comma-separated list of "kafka://host:port"
        - name: KAFKA_URL
          value: kafka://analytickit-analytickit-kafka:9092
        

        # Object Storage env variables        
        
        

        # Redis env variables        
        
        - name: ANALYTICKIT_REDIS_HOST
          value: analytickit-analytickit-redis-master
        
        - name: ANALYTICKIT_REDIS_PORT
          value: "6379"

        # statsd env variables        

        - name: DISABLE_SECURE_SSL_REDIRECT
          value: '1'
        - name: IS_BEHIND_PROXY
          value: '1'
        # PostHog app settings        
        - name: SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: analytickit
              key: analytickit-secret
        - name: SITE_URL
          value: dpa.analytickit.com
        - name: DEPLOYMENT
          value: helm_aws_ha
        - name: CAPTURE_INTERNAL_METRICS
          value: "true"
        - name: HELM_INSTALL_INFO
          value: "{\"chart_version\":\"0.0.1\",\"cloud\":\"aws\",\"deployment_type\":\"helm\",\"hostname\":null,\"ingress_type\":\"\",\"kube_version\":\"v1.23.13-eks-fb459a0\",\"operation\":\"install\",\"release_name\":\"analytickit\",\"release_revision\":1}"
        - name: LOGGING_FORMATTER_NAME
          value: json        
        - name: SENTRY_DSN
          value: 
        - name: SAML_ENTITY_ID
          value: ""
        - name: SAML_ACS_URL
          value: ""
        - name: SAML_X509_CERT
          value: ""
        - name: SAML_ATTR_PERMANENT_ID
          value: ""
        - name: SAML_ATTR_FIRST_NAME
          value: ""
        - name: SAML_ATTR_LAST_NAME
          value: ""
        - name: SAML_ATTR_EMAIL
          value: ""
        - name: PRIMARY_DB
          value: clickhouse
        
        - name: ANALYTICKIT_POSTGRES_HOST
          value: analytickit-pgbouncer
        - name: ANALYTICKIT_POSTGRES_PORT
          value: "6543"
        - name: ANALYTICKIT_DB_USER
          value: "postgres"
        - name: ANALYTICKIT_DB_NAME
          value: "analytickit"
        - name: ANALYTICKIT_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: analytickit-analytickit-postgresql
              key: "postgresql-password"
        - name: USING_PGBOUNCER
          value: 'true'
        - name: CLICKHOUSE_HOST
          value: clickhouse-analytickit
        - name: CLICKHOUSE_CLUSTER
          value: "analytickit"
        - name: CLICKHOUSE_DATABASE
          value: "analytickit"
        - name: CLICKHOUSE_USER
          value: "admin"
        - name: CLICKHOUSE_PASSWORD
          value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
        - name: CLICKHOUSE_SECURE
          value: "false"
        - name: CLICKHOUSE_VERIFY
          value: "false"
        
        - name: EMAIL_HOST
          value: ""
        - name: EMAIL_PORT
          value: ""
        - name: EMAIL_HOST_USER
          value: ""
        - name: EMAIL_HOST_PASSWORD
          valueFrom:
            secretKeyRef:
              name: analytickit
              key: "smtp-password"
        - name: EMAIL_USE_TLS
          value: "true"
        - name: EMAIL_USE_SSL
          value: "false"
        - name: DEFAULT_FROM_EMAIL
          value: 
        - name: SOCIAL_AUTH_GOOGLE_OAUTH2_KEY
          value: null
        - name: SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET
          value: null
        - name: SOCIAL_AUTH_GOOGLE_OAUTH2_WHITELISTED_DOMAINS
          value: analytickit.com
        lifecycle:
            preStop:
                exec:
                    command: [
                        "sh", "-c",
                        "(echo '{\"event\": \"preStop_started\"}'; sleep 10; echo '{\"event\": \"preStop_ended\"}') > /proc/1/fd/1"
                    ]

        livenessProbe:
          httpGet:
            path: /_livez
            port: 8000
            scheme: HTTP
          failureThreshold: 3
          initialDelaySeconds: 0
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 2
        readinessProbe:
          httpGet:
            # For readiness, we want to use the checks specific to the web
            # role, which may be a subset of all the apps dependencies
            path: /_readyz?role=web
            port: 8000
            scheme: HTTP
          failureThreshold: 3
          initialDelaySeconds: 0
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        startupProbe:
          httpGet:
            # For startup, we want to make sure that everything is in place,
            # including postgres. This does however mean we would not be able to
            # deploy new releases when we have a postgres outage
            path: /_readyz
            port: 8000
            scheme: HTTP
          failureThreshold: 6
          initialDelaySeconds: 0
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
            {}
      initContainers:        
        - name: wait-for-service-dependencies
          image: busybox:1.34
          imagePullPolicy: IfNotPresent
          env:
            - name: CLICKHOUSE_HOST
              value: clickhouse-analytickit
            - name: CLICKHOUSE_CLUSTER
              value: "analytickit"
            - name: CLICKHOUSE_DATABASE
              value: "analytickit"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
            - name: CLICKHOUSE_SECURE
              value: "false"
            - name: CLICKHOUSE_VERIFY
              value: "false"
          command:
            - /bin/sh
            - -c
            - >
                
                until (
                    NODES_COUNT=$(wget -qO- \
                        "http://$CLICKHOUSE_USER:$CLICKHOUSE_PASSWORD@clickhouse-analytickit.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local:8123" \
                        --post-data "SELECT count() FROM clusterAllReplicas('analytickit', system, one)"
                    )
                    test ! -z $NODES_COUNT && test $NODES_COUNT -eq 1
                );
                do
                    echo "waiting for all ClickHouse nodes to be available"; sleep 1;
                done
                
        
                until (nc -vz "analytickit-pgbouncer.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" 6543);
                do
                    echo "waiting for PgBouncer"; sleep 1;
                done
        
                
                until (nc -vz "analytickit-analytickit-postgresql.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" 5432);
                do
                    echo "waiting for PostgreSQL"; sleep 1;
                done
                
        
                
                until (nc -vz "analytickit-analytickit-redis-master.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" "6379");
                do
                    echo "waiting for Redis"; sleep 1;
                done
                
        
                
        
                KAFKA_BROKERS="analytickit-analytickit-kafka:9092"
        
                KAFKA_HOST=$(echo $KAFKA_BROKERS | cut -f1 -d:)
                KAFKA_PORT=$(echo $KAFKA_BROKERS | cut -f2 -d:)
        
                until (nc -vz "$KAFKA_HOST.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" $KAFKA_PORT);
                do
                    echo "waiting for Kafka"; sleep 1;
                done
                        
        - name: wait-for-migrations
          image: "analytickit/analytickit:release-1.41.3"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - >
                until (python manage.py migrate --check);
                do
                    echo "Waiting for PostgreSQL database migrations to be completed"; sleep 1;
                done
        
                until (python manage.py migrate_clickhouse --check);
                do
                    echo "Waiting for ClickHouse database migrations to be completed";
                    sleep 1;
                done
          env:
        
          # PostgreSQL configuration
          
          - name: ANALYTICKIT_POSTGRES_HOST
            value: analytickit-pgbouncer
          - name: ANALYTICKIT_POSTGRES_PORT
            value: "6543"
          - name: ANALYTICKIT_DB_USER
            value: "postgres"
          - name: ANALYTICKIT_DB_NAME
            value: "analytickit"
          - name: ANALYTICKIT_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: analytickit-analytickit-postgresql
                key: "postgresql-password"
          - name: USING_PGBOUNCER
            value: 'true'
        
          # Redis env variables  
          
          - name: ANALYTICKIT_REDIS_HOST
            value: analytickit-analytickit-redis-master
          
          - name: ANALYTICKIT_REDIS_PORT
            value: "6379"
        
          # ClickHouse env variables
          - name: CLICKHOUSE_HOST
            value: clickhouse-analytickit
          - name: CLICKHOUSE_CLUSTER
            value: "analytickit"
          - name: CLICKHOUSE_DATABASE
            value: "analytickit"
          - name: CLICKHOUSE_USER
            value: "admin"
          - name: CLICKHOUSE_PASSWORD
            value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
          - name: CLICKHOUSE_SECURE
            value: "false"
          - name: CLICKHOUSE_VERIFY
            value: "false"
        
          # PostHog app settings
          
          - name: SECRET_KEY
            valueFrom:
              secretKeyRef:
                name: analytickit
                key: analytickit-secret
          - name: SITE_URL
            value: dpa.analytickit.com
          - name: DEPLOYMENT
            value: helm_aws_ha
          - name: CAPTURE_INTERNAL_METRICS
            value: "true"
          - name: HELM_INSTALL_INFO
            value: "{\"chart_version\":\"0.0.1\",\"cloud\":\"aws\",\"deployment_type\":\"helm\",\"hostname\":null,\"ingress_type\":\"\",\"kube_version\":\"v1.23.13-eks-fb459a0\",\"operation\":\"install\",\"release_name\":\"analytickit\",\"release_revision\":1}"
          - name: LOGGING_FORMATTER_NAME
            value: json
          
          - name: SENTRY_DSN
            value: 
        
          # Global ENV variables
---
# Source: analytickit/templates/worker-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: analytickit-worker
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
  annotations:
    
    "meta.helm.sh/release-name": "analytickit"
    "meta.helm.sh/release-namespace": "default"
spec:
  selector:
    matchLabels:
        app: analytickit
        release: "analytickit"
        role: worker
  replicas: 1
  template:
    metadata:
      annotations:
        checksum/secrets.yaml: 802a7e665c96627f72c9ebdc2081b7ab8f63c9f7eb0a711d6b34b53d4a426318
      labels:
        app: analytickit
        release: "analytickit"
        role: worker
    spec:
      serviceAccountName: analytickit
      containers:
      - name: analytickit-workers
        image: "analytickit/analytickit:release-1.41.3"
        imagePullPolicy: IfNotPresent
        command:
          - ./bin/docker-worker-celery
          - --with-scheduler
        ports:
        - containerPort: 8000
        env:
        # Kafka env variables        
        # Used by PostHog/plugin-server. There is no specific reason for the difference. Expected format: comma-separated list of "host:port"
        - name: KAFKA_HOSTS
          value: analytickit-analytickit-kafka:9092
        
        # Used by PostHog/web. There is no specific reason for the difference. Expected format: comma-separated list of "kafka://host:port"
        - name: KAFKA_URL
          value: kafka://analytickit-analytickit-kafka:9092
        

        # Object Storage env variables        
        
        

        # Redis env variables        
        
        - name: ANALYTICKIT_REDIS_HOST
          value: analytickit-analytickit-redis-master
        
        - name: ANALYTICKIT_REDIS_PORT
          value: "6379"

        # statsd env variables        

        - name: PRIMARY_DB
          value: clickhouse        
        - name: SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: analytickit
              key: analytickit-secret
        - name: SITE_URL
          value: dpa.analytickit.com
        - name: DEPLOYMENT
          value: helm_aws_ha
        - name: CAPTURE_INTERNAL_METRICS
          value: "true"
        - name: HELM_INSTALL_INFO
          value: "{\"chart_version\":\"0.0.1\",\"cloud\":\"aws\",\"deployment_type\":\"helm\",\"hostname\":null,\"ingress_type\":\"\",\"kube_version\":\"v1.23.13-eks-fb459a0\",\"operation\":\"install\",\"release_name\":\"analytickit\",\"release_revision\":1}"
        - name: LOGGING_FORMATTER_NAME
          value: json        
        - name: SENTRY_DSN
          value: 
        
        - name: ANALYTICKIT_POSTGRES_HOST
          value: analytickit-pgbouncer
        - name: ANALYTICKIT_POSTGRES_PORT
          value: "6543"
        - name: ANALYTICKIT_DB_USER
          value: "postgres"
        - name: ANALYTICKIT_DB_NAME
          value: "analytickit"
        - name: ANALYTICKIT_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: analytickit-analytickit-postgresql
              key: "postgresql-password"
        - name: USING_PGBOUNCER
          value: 'true'
        - name: CLICKHOUSE_HOST
          value: clickhouse-analytickit
        - name: CLICKHOUSE_CLUSTER
          value: "analytickit"
        - name: CLICKHOUSE_DATABASE
          value: "analytickit"
        - name: CLICKHOUSE_USER
          value: "admin"
        - name: CLICKHOUSE_PASSWORD
          value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
        - name: CLICKHOUSE_SECURE
          value: "false"
        - name: CLICKHOUSE_VERIFY
          value: "false"
        
        - name: EMAIL_HOST
          value: ""
        - name: EMAIL_PORT
          value: ""
        - name: EMAIL_HOST_USER
          value: ""
        - name: EMAIL_HOST_PASSWORD
          valueFrom:
            secretKeyRef:
              name: analytickit
              key: "smtp-password"
        - name: EMAIL_USE_TLS
          value: "true"
        - name: EMAIL_USE_SSL
          value: "false"
        - name: DEFAULT_FROM_EMAIL
          value: 
        resources:
            {}
      initContainers:        
        - name: wait-for-service-dependencies
          image: busybox:1.34
          imagePullPolicy: IfNotPresent
          env:
            - name: CLICKHOUSE_HOST
              value: clickhouse-analytickit
            - name: CLICKHOUSE_CLUSTER
              value: "analytickit"
            - name: CLICKHOUSE_DATABASE
              value: "analytickit"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
            - name: CLICKHOUSE_SECURE
              value: "false"
            - name: CLICKHOUSE_VERIFY
              value: "false"
          command:
            - /bin/sh
            - -c
            - >
                
                until (
                    NODES_COUNT=$(wget -qO- \
                        "http://$CLICKHOUSE_USER:$CLICKHOUSE_PASSWORD@clickhouse-analytickit.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local:8123" \
                        --post-data "SELECT count() FROM clusterAllReplicas('analytickit', system, one)"
                    )
                    test ! -z $NODES_COUNT && test $NODES_COUNT -eq 1
                );
                do
                    echo "waiting for all ClickHouse nodes to be available"; sleep 1;
                done
                
        
                until (nc -vz "analytickit-pgbouncer.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" 6543);
                do
                    echo "waiting for PgBouncer"; sleep 1;
                done
        
                
                until (nc -vz "analytickit-analytickit-postgresql.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" 5432);
                do
                    echo "waiting for PostgreSQL"; sleep 1;
                done
                
        
                
                until (nc -vz "analytickit-analytickit-redis-master.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" "6379");
                do
                    echo "waiting for Redis"; sleep 1;
                done
                
        
                
        
                KAFKA_BROKERS="analytickit-analytickit-kafka:9092"
        
                KAFKA_HOST=$(echo $KAFKA_BROKERS | cut -f1 -d:)
                KAFKA_PORT=$(echo $KAFKA_BROKERS | cut -f2 -d:)
        
                until (nc -vz "$KAFKA_HOST.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" $KAFKA_PORT);
                do
                    echo "waiting for Kafka"; sleep 1;
                done
                        
        - name: wait-for-migrations
          image: "analytickit/analytickit:release-1.41.3"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - >
                until (python manage.py migrate --check);
                do
                    echo "Waiting for PostgreSQL database migrations to be completed"; sleep 1;
                done
        
                until (python manage.py migrate_clickhouse --check);
                do
                    echo "Waiting for ClickHouse database migrations to be completed";
                    sleep 1;
                done
          env:
        
          # PostgreSQL configuration
          
          - name: ANALYTICKIT_POSTGRES_HOST
            value: analytickit-pgbouncer
          - name: ANALYTICKIT_POSTGRES_PORT
            value: "6543"
          - name: ANALYTICKIT_DB_USER
            value: "postgres"
          - name: ANALYTICKIT_DB_NAME
            value: "analytickit"
          - name: ANALYTICKIT_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: analytickit-analytickit-postgresql
                key: "postgresql-password"
          - name: USING_PGBOUNCER
            value: 'true'
        
          # Redis env variables  
          
          - name: ANALYTICKIT_REDIS_HOST
            value: analytickit-analytickit-redis-master
          
          - name: ANALYTICKIT_REDIS_PORT
            value: "6379"
        
          # ClickHouse env variables
          - name: CLICKHOUSE_HOST
            value: clickhouse-analytickit
          - name: CLICKHOUSE_CLUSTER
            value: "analytickit"
          - name: CLICKHOUSE_DATABASE
            value: "analytickit"
          - name: CLICKHOUSE_USER
            value: "admin"
          - name: CLICKHOUSE_PASSWORD
            value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
          - name: CLICKHOUSE_SECURE
            value: "false"
          - name: CLICKHOUSE_VERIFY
            value: "false"
        
          # PostHog app settings
          
          - name: SECRET_KEY
            valueFrom:
              secretKeyRef:
                name: analytickit
                key: analytickit-secret
          - name: SITE_URL
            value: dpa.analytickit.com
          - name: DEPLOYMENT
            value: helm_aws_ha
          - name: CAPTURE_INTERNAL_METRICS
            value: "true"
          - name: HELM_INSTALL_INFO
            value: "{\"chart_version\":\"0.0.1\",\"cloud\":\"aws\",\"deployment_type\":\"helm\",\"hostname\":null,\"ingress_type\":\"\",\"kube_version\":\"v1.23.13-eks-fb459a0\",\"operation\":\"install\",\"release_name\":\"analytickit\",\"release_revision\":1}"
          - name: LOGGING_FORMATTER_NAME
            value: json
          
          - name: SENTRY_DSN
            value: 
        
          # Global ENV variables
---
# Source: analytickit/charts/kafka/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: analytickit-analytickit-kafka
  labels:
    app.kubernetes.io/name: analytickit-kafka
    helm.sh/chart: kafka-14.9.3
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: analytickit-kafka
      app.kubernetes.io/instance: analytickit
      app.kubernetes.io/component: kafka
  serviceName: analytickit-analytickit-kafka-headless
  updateStrategy:
    type: "RollingUpdate"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: analytickit-kafka
        helm.sh/chart: kafka-14.9.3
        app.kubernetes.io/instance: analytickit
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: kafka
    spec:
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: analytickit-kafka
                    app.kubernetes.io/instance: analytickit
                    app.kubernetes.io/component: kafka
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
      serviceAccountName: analytickit-analytickit-kafka
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:2.8.1-debian-10-r99
          imagePullPolicy: "IfNotPresent"
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_CFG_ZOOKEEPER_CONNECT
              value: analytickit-analytickit-zookeeper:2181
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: "INTERNAL"
            - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
              value: "INTERNAL:PLAINTEXT,CLIENT:PLAINTEXT"
            - name: KAFKA_CFG_LISTENERS
              value: "INTERNAL://:9093,CLIENT://:9092"
            - name: KAFKA_CFG_ADVERTISED_LISTENERS
              value: "INTERNAL://$(MY_POD_NAME).analytickit-analytickit-kafka-headless.default.svc.cluster.local:9093,CLIENT://$(MY_POD_NAME).analytickit-analytickit-kafka-headless.default.svc.cluster.local:9092"
            - name: ALLOW_PLAINTEXT_LISTENER
              value: "yes"
            - name: KAFKA_VOLUME_DIR
              value: "/bitnami/kafka"
            - name: KAFKA_LOG_DIR
              value: "/opt/bitnami/kafka/logs"
            - name: KAFKA_CFG_DELETE_TOPIC_ENABLE
              value: "false"
            - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
              value: "true"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx1024m -Xms1024m"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES
              value: "10000"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MS
              value: "1000"
            - name: KAFKA_CFG_LOG_RETENTION_BYTES
              value: "15000000000"
            - name: KAFKA_CFG_LOG_RETENTION_CHECK_INTERVALS_MS
              value: "300000"
            - name: KAFKA_CFG_LOG_RETENTION_HOURS
              value: "24"
            - name: KAFKA_CFG_MESSAGE_MAX_BYTES
              value: "1000012"
            - name: KAFKA_CFG_LOG_SEGMENT_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_DIRS
              value: "/bitnami/kafka/data"
            - name: KAFKA_CFG_DEFAULT_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR
              value: "1"
            - name: KAFKA_CFG_NUM_IO_THREADS
              value: "8"
            - name: KAFKA_CFG_NUM_NETWORK_THREADS
              value: "3"
            - name: KAFKA_CFG_NUM_PARTITIONS
              value: "1"
            - name: KAFKA_CFG_NUM_RECOVERY_THREADS_PER_DATA_DIR
              value: "1"
            - name: KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES
              value: "104857600"
            - name: KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_ZOOKEEPER_CONNECTION_TIMEOUT_MS
              value: "6000"
            - name: KAFKA_CFG_AUTHORIZER_CLASS_NAME
              value: ""
            - name: KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND
              value: "true"
            - name: KAFKA_CFG_SUPER_USERS
              value: "User:admin"
          ports:
            - name: kafka-client
              containerPort: 9092
            - name: kafka-internal
              containerPort: 9093
          livenessProbe:
            tcpSocket:
              port: kafka-client
            initialDelaySeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
          readinessProbe:
            tcpSocket:
              port: kafka-client
            initialDelaySeconds: 5
            timeoutSeconds: 5
            failureThreshold: 6
            periodSeconds: 10
            successThreshold: 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
      volumes:
        - name: scripts
          configMap:
            name: analytickit-analytickit-kafka-scripts
            defaultMode: 0755
        - name: logs
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "20Gi"
---
# Source: analytickit/charts/postgresql/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: analytickit-analytickit-postgresql
  labels:
    app: analytickit-postgresql
    chart: postgresql-8.6.1
    release: "analytickit"
    heritage: "Helm"
spec:
  serviceName: analytickit-analytickit-postgresql-headless
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: analytickit-postgresql
      release: "analytickit"
      role: master
  template:
    metadata:
      name: analytickit-analytickit-postgresql
      labels:
        app: analytickit-postgresql
        chart: postgresql-8.6.1
        release: "analytickit"
        heritage: "Helm"
        role: master
    spec:      
      securityContext:
        fsGroup: 1001
      initContainers:
        # - name: do-something
        #   image: busybox
        #   command: ['do', 'something']
        
      containers:
        - name: analytickit-analytickit-postgresql
          image: docker.io/bitnami/postgresql:11.7.0-debian-10-r9
          imagePullPolicy: "IfNotPresent"
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            - name: POSTGRES_USER
              value: "postgres"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: analytickit-analytickit-postgresql
                  key: postgresql-password
            - name: POSTGRES_DB
              value: "analytickit"
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -d "analytickit" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "postgres" -d "analytickit" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
              subPath: 
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "10Gi"
---
# Source: analytickit/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: analytickit-analytickit-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: analytickit-redis
    helm.sh/chart: redis-16.8.9
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: analytickit-redis
      app.kubernetes.io/instance: analytickit
      app.kubernetes.io/component: master
  serviceName: analytickit-analytickit-redis-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: analytickit-redis
        helm.sh/chart: redis-16.8.9
        app.kubernetes.io/instance: analytickit
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: e4e8c820075f6ad76b3dded956caee8fdb47b411c2ffee70a85f637c8fdb7a90
        checksum/health: 441c4a54c948d165198b0862685ed1fd6a3942d948ce5dab6e40e062eae5ceba
        checksum/scripts: 7ad6159883804494b10427b65c374795c300fd3c067eea30dc2c06542c863ebc
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: analytickit-analytickit-redis
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: analytickit-redis
                    app.kubernetes.io/instance: analytickit
                    app.kubernetes.io/component: master
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:6.2.7-debian-10-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
              subPath: 
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc/
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: start-scripts
          configMap:
            name: analytickit-analytickit-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: analytickit-analytickit-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: analytickit-analytickit-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
        - name: tmp
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: analytickit-redis
          app.kubernetes.io/instance: analytickit
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "5Gi"
---
# Source: analytickit/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: analytickit-analytickit-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/name: analytickit-zookeeper
    helm.sh/chart: zookeeper-7.0.5
    app.kubernetes.io/instance: analytickit
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  serviceName: analytickit-analytickit-zookeeper-headless
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: analytickit-zookeeper
      app.kubernetes.io/instance: analytickit
      app.kubernetes.io/component: zookeeper
  template:
    metadata:
      name: analytickit-analytickit-zookeeper
      labels:
        app.kubernetes.io/name: analytickit-zookeeper
        helm.sh/chart: zookeeper-7.0.5
        app.kubernetes.io/instance: analytickit
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      
      serviceAccountName: default
      securityContext:
        fsGroup: 1001
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: analytickit-zookeeper
                    app.kubernetes.io/instance: analytickit
                    app.kubernetes.io/component: zookeeper
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.7.0-debian-10-r70
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - bash
            - -ec
            - |
                # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
                # check ZOO_SERVER_ID in persistent volume via myid
                # if not present, set based on POD hostname
                if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
                  export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
                else
                  HOSTNAME=`hostname -s`
                  if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
                    ORD=${BASH_REMATCH[2]}
                    export ZOO_SERVER_ID=$((ORD + 1 ))
                  else
                    echo "Failed to get index from hostname $HOST"
                    exit 1
                  fi
                fi
                exec /entrypoint.sh /run.sh
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "1"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: analytickit-analytickit-zookeeper-0.analytickit-analytickit-zookeeper-headless.default.svc.cluster.local:2888:3888::1 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: data
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: analytickit/templates/migrate.job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: analytickit-migrate-2022-11-14-14-22-33
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
  annotations:
    
    "meta.helm.sh/release-name": "analytickit"
    "meta.helm.sh/release-namespace": "default"
spec:
  template:
    metadata:
      name: analytickit-migrate-2022-11-14-14-22-33
      annotations:
        checksum/secrets.yaml: b1411bb4edd330163c4acf1e550e20eff5808b906cdf9503fbf5e3f745b7a328
      labels:
        app: analytickit
        release: "analytickit"
    spec:
      restartPolicy: Never
      containers:
      - name: migrate-job
        image: "analytickit/analytickit:release-1.41.3"
        imagePullPolicy: IfNotPresent
        command:
          - /bin/sh
          - -c
          - |
            set -e
            python manage.py notify_helm_install || true
            ./bin/migrate

        env:
        # Kafka env variables        
        # Used by PostHog/plugin-server. There is no specific reason for the difference. Expected format: comma-separated list of "host:port"
        - name: KAFKA_HOSTS
          value: analytickit-analytickit-kafka:9092
        
        # Used by PostHog/web. There is no specific reason for the difference. Expected format: comma-separated list of "kafka://host:port"
        - name: KAFKA_URL
          value: kafka://analytickit-analytickit-kafka:9092
        

        # Object Storage env variables        
        
        

        # Redis env variables        
        
        - name: ANALYTICKIT_REDIS_HOST
          value: analytickit-analytickit-redis-master
        
        - name: ANALYTICKIT_REDIS_PORT
          value: "6379"
        # PostHog app settings        
        - name: SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: analytickit
              key: analytickit-secret
        - name: SITE_URL
          value: dpa.analytickit.com
        - name: DEPLOYMENT
          value: helm_aws_ha
        - name: CAPTURE_INTERNAL_METRICS
          value: "true"
        - name: HELM_INSTALL_INFO
          value: "{\"chart_version\":\"0.0.1\",\"cloud\":\"aws\",\"deployment_type\":\"helm\",\"hostname\":null,\"ingress_type\":\"\",\"kube_version\":\"v1.23.13-eks-fb459a0\",\"operation\":\"install\",\"release_name\":\"analytickit\",\"release_revision\":1}"
        - name: LOGGING_FORMATTER_NAME
          value: json        
        - name: SENTRY_DSN
          value: 
        - name: PRIMARY_DB
          value: clickhouse
        
        # Connect directly to postgres (without pgbouncer) to avoid statement_timeout for longer-running queries
        - name: ANALYTICKIT_POSTGRES_HOST
          value: analytickit-analytickit-postgresql
        - name: ANALYTICKIT_POSTGRES_PORT
          value: "5432"
        - name: ANALYTICKIT_DB_USER
          value: "postgres"
        - name: ANALYTICKIT_DB_NAME
          value: "analytickit"
        - name: ANALYTICKIT_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: analytickit-analytickit-postgresql
              key: "postgresql-password"
        - name: USING_PGBOUNCER
          value: 'false'
        - name: CLICKHOUSE_HOST
          value: clickhouse-analytickit
        - name: CLICKHOUSE_CLUSTER
          value: "analytickit"
        - name: CLICKHOUSE_DATABASE
          value: "analytickit"
        - name: CLICKHOUSE_USER
          value: "admin"
        - name: CLICKHOUSE_PASSWORD
          value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
        - name: CLICKHOUSE_SECURE
          value: "false"
        - name: CLICKHOUSE_VERIFY
          value: "false"
        
        - name: EMAIL_HOST
          value: ""
        - name: EMAIL_PORT
          value: ""
        - name: EMAIL_HOST_USER
          value: ""
        - name: EMAIL_HOST_PASSWORD
          valueFrom:
            secretKeyRef:
              name: analytickit
              key: "smtp-password"
        - name: EMAIL_USE_TLS
          value: "true"
        - name: EMAIL_USE_SSL
          value: "false"
        - name: DEFAULT_FROM_EMAIL
          value: 
        resources:
          {}
      initContainers:        
        - name: wait-for-service-dependencies
          image: busybox:1.34
          imagePullPolicy: IfNotPresent
          env:
            - name: CLICKHOUSE_HOST
              value: clickhouse-analytickit
            - name: CLICKHOUSE_CLUSTER
              value: "analytickit"
            - name: CLICKHOUSE_DATABASE
              value: "analytickit"
            - name: CLICKHOUSE_USER
              value: "admin"
            - name: CLICKHOUSE_PASSWORD
              value: "a1f31e03-c88e-4ca6-a2df-ad49183d15d9"
            - name: CLICKHOUSE_SECURE
              value: "false"
            - name: CLICKHOUSE_VERIFY
              value: "false"
          command:
            - /bin/sh
            - -c
            - >
                
                until (
                    NODES_COUNT=$(wget -qO- \
                        "http://$CLICKHOUSE_USER:$CLICKHOUSE_PASSWORD@clickhouse-analytickit.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local:8123" \
                        --post-data "SELECT count() FROM clusterAllReplicas('analytickit', system, one)"
                    )
                    test ! -z $NODES_COUNT && test $NODES_COUNT -eq 1
                );
                do
                    echo "waiting for all ClickHouse nodes to be available"; sleep 1;
                done
                
        
                until (nc -vz "analytickit-pgbouncer.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" 6543);
                do
                    echo "waiting for PgBouncer"; sleep 1;
                done
        
                
                until (nc -vz "analytickit-analytickit-postgresql.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" 5432);
                do
                    echo "waiting for PostgreSQL"; sleep 1;
                done
                
        
                
                until (nc -vz "analytickit-analytickit-redis-master.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" "6379");
                do
                    echo "waiting for Redis"; sleep 1;
                done
                
        
                
        
                KAFKA_BROKERS="analytickit-analytickit-kafka:9092"
        
                KAFKA_HOST=$(echo $KAFKA_BROKERS | cut -f1 -d:)
                KAFKA_PORT=$(echo $KAFKA_BROKERS | cut -f2 -d:)
        
                until (nc -vz "$KAFKA_HOST.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local" $KAFKA_PORT);
                do
                    echo "waiting for Kafka"; sleep 1;
                done
---
# Source: analytickit/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: analytickit
  labels:
    
    "app.kubernetes.io/name": "analytickit"
    "app.kubernetes.io/instance": "analytickit"
    "app.kubernetes.io/managed-by": "Helm"
    "helm.sh/chart": "analytickit-0.0.1"
  annotations:
    
    "meta.helm.sh/release-name": "analytickit"
    "meta.helm.sh/release-namespace": "default"
spec:
  rules:
    - http:
        paths:
          - pathType: Prefix
            path: "/"
            backend:
              service:
                name: analytickit-web
                port:
                  number: 8000
          - pathType: Prefix
            path: "/batch"
            backend: &INGESTION
              service:
                name: analytickit-events
                port:
                  number: 8000
          - pathType: Prefix
            path: "/capture"
            backend: *INGESTION
          - pathType: Prefix
            path: "/decide"
            backend: *INGESTION
          - pathType: Prefix
            path: "/e"
            backend: *INGESTION
          - pathType: Prefix
            path: "/engage"
            backend: *INGESTION
          - pathType: Prefix
            path: "/track"
            backend: *INGESTION
          - pathType: Prefix
            path: "/s"
            backend: *INGESTION
---
# Source: analytickit/templates/eventrouter.yaml
#
# If enabled, we add a Deployment for
# https://github.com/vmware-archive/eventrouter as per
# https://grafana.com/blog/2020/07/21/loki-tutorial-how-to-send-logs-from-eks-with-promtail-to-get-full-visibility-in-grafana/
#
# It looks like a dead project which is unfortunate but appears to work. There
# is a Grafana Agent integration to pull Kubernetes events but this appears to
# be experimental at the time of writing:
# https://grafana.com/docs/agent/latest/configuration/integrations/integrations-next/eventhandler-config/
#
---
# Source: analytickit/templates/grafana-annotation-post.job.yaml
#
# This job gets installed only if Grafana, Loki and Promtail are enabled.
#
# It's an ephemeral container running at the start and end of each Helm
# deploy and it's used to log at stdout the Helm revision we are
# installing / we finished installing.
#
# This datapoint is useful and can be very helpful as annotation
# in Grafana dashboard.
#
---
# Source: analytickit/templates/grafana-annotation-pre.job.yaml
#
# This job gets installed only if Grafana, Loki and Promtail are enabled.
#
# It's an ephemeral container running at the start and end of each Helm
# deploy and it's used to log at stdout the Helm revision we are
# installing / we finished installing.
#
# This datapoint is useful and can be very helpful as annotation
# in Grafana dashboard.
#
---
# Source: analytickit/templates/clickhouse_instance.yaml
apiVersion: "clickhouse.altinity.com/v1"
kind: "ClickHouseInstallation"
metadata:
  name: "posthog"
spec:
  configuration:
    users:
      admin/password: a1f31e03-c88e-4ca6-a2df-ad49183d15d9
      admin/networks/ip:
        - "10.0.0.0/8"
        - "172.16.0.0/12"
        - "192.168.0.0/16"
      admin/profile: default
      admin/quota: default
    profiles:
      default/allow_experimental_window_functions: "1"
      default/allow_nondeterministic_mutations: "1"

    clusters:
      - name: "analytickit"
        templates:
          podTemplate: pod-template
          clusterServiceTemplate: service-template
          dataVolumeClaimTemplate: data-volumeclaim-template
        layout:
          replicasCount: 1
          shardsCount: 1

    settings:
      default_database: analytickit
      format_schema_path: /etc/clickhouse-server/config.d/

    files:
      events.proto: |
        syntax = "proto3";
        message Event {
          string uuid = 1;
          string event = 2;
          string properties = 3;
          string timestamp = 4;
          uint64 team_id = 5;
          string distinct_id = 6;
          string created_at = 7;
          string elements_chain = 8;
        }

    zookeeper:
      nodes:
        - host: analytickit-analytickit-zookeeper
          port: 2181

  templates:
    podTemplates:
      - name: pod-template
        spec:
          volumes:
            - name: data-volumeclaim-template
              persistentVolumeClaim:
                claimName: data-volumeclaim-template
          securityContext:
            fsGroup: 101
            runAsGroup: 101
            runAsUser: 101
          containers:
            - name: clickhouse
              # KEEP CLICKHOUSE-SERVER VERSION IN SYNC WITH
              # https://github.com/PostHog/posthog/tree/master/ee/docker-compose.ch.yml#L17
              image: "clickhouse/clickhouse-server:22.3.13.80"
              command:
                - /bin/bash
                - -c
                - /usr/bin/clickhouse-server --config-file=/etc/clickhouse-server/config.xml

              ports:
                - name: http
                  containerPort: 8123
                - name: client
                  containerPort: 9000
                - name: interserver
                  containerPort: 9009
              volumeMounts:
                - name: data-volumeclaim-template
                  mountPath: /var/lib/clickhouse

    serviceTemplates:
      - name: service-template
        generateName: clickhouse-analytickit
        spec:
          ports:
            - name: http
              port: 8123
            - name: tcp
              port: 9000
          type: ClusterIP
    volumeClaimTemplates:
      - name: data-volumeclaim-template
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: "20Gi"

NOTES:
CHART NAME: analytickit
CHART VERSION: 0.0.1
APP VERSION: 0.0.1

** Please be patient while the chart is being deployed **

To access your PostHog site from outside the cluster follow the steps below:
1. Your application will be hosted at an ingress IP because hostname was not supplied. Run these commands to get your installation location:
export INGRESS_IP=$(kubectl get --namespace default ingress posthog -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
echo "Visit http://$INGRESS_IP to use PostHog\!"
